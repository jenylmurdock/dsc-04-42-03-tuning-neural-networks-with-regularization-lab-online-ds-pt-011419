{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some packages/modules you plan to use\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "\n",
    "print (data.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_results type: <class 'numpy.ndarray'>\n",
      "Found 21608 unique tokens.\n",
      "Dimensions of our coded results: (10000, 2000)\n"
     ]
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "# returns a numpy array\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index #Useful if we wish to decode (more explanation below)\n",
    "\n",
    "# Tokens are the number of unique words across the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) #Our coded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "product = df[\"Product\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation for label encoder\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "\n",
    "# transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a neural network using Keras as described above\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "#2 hidden layers\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 3s 465us/step - loss: 1.9541 - acc: 0.1544 - val_loss: 1.9513 - val_acc: 0.1510\n",
      "Epoch 2/120\n",
      "1792/7500 [======>.......................] - ETA: 1s - loss: 1.9387 - acc: 0.1758"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenylmurdock/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.140310). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.9369 - acc: 0.1688 - val_loss: 1.9385 - val_acc: 0.1690\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 1.9221 - acc: 0.1853 - val_loss: 1.9250 - val_acc: 0.1850\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.9051 - acc: 0.2124 - val_loss: 1.9086 - val_acc: 0.2170\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.8850 - acc: 0.2479 - val_loss: 1.8892 - val_acc: 0.2580\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 1.8622 - acc: 0.2808 - val_loss: 1.8674 - val_acc: 0.2840\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.8370 - acc: 0.3107 - val_loss: 1.8435 - val_acc: 0.3060\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.8095 - acc: 0.3299 - val_loss: 1.8169 - val_acc: 0.3180\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 1.7798 - acc: 0.3517 - val_loss: 1.7870 - val_acc: 0.3340\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.7474 - acc: 0.3683 - val_loss: 1.7536 - val_acc: 0.3490\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.7118 - acc: 0.3879 - val_loss: 1.7157 - val_acc: 0.3730\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.6727 - acc: 0.4076 - val_loss: 1.6738 - val_acc: 0.3960\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.6299 - acc: 0.4299 - val_loss: 1.6280 - val_acc: 0.4120\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.5841 - acc: 0.4529 - val_loss: 1.5790 - val_acc: 0.4490\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.5362 - acc: 0.4792 - val_loss: 1.5273 - val_acc: 0.4690\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.4867 - acc: 0.5083 - val_loss: 1.4754 - val_acc: 0.5070\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.4373 - acc: 0.5395 - val_loss: 1.4261 - val_acc: 0.5290\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 1.3890 - acc: 0.5649 - val_loss: 1.3778 - val_acc: 0.5500\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.3419 - acc: 0.5876 - val_loss: 1.3318 - val_acc: 0.5860\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.2968 - acc: 0.6073 - val_loss: 1.2883 - val_acc: 0.6090\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.2524 - acc: 0.6283 - val_loss: 1.2461 - val_acc: 0.6250\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.2103 - acc: 0.6447 - val_loss: 1.2050 - val_acc: 0.6400\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.1695 - acc: 0.6579 - val_loss: 1.1682 - val_acc: 0.6460\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.1305 - acc: 0.6712 - val_loss: 1.1308 - val_acc: 0.6570\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.0926 - acc: 0.6800 - val_loss: 1.0950 - val_acc: 0.6620\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.0570 - acc: 0.6913 - val_loss: 1.0658 - val_acc: 0.6620\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 1.0231 - acc: 0.6981 - val_loss: 1.0327 - val_acc: 0.6760\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 0.9901 - acc: 0.7049 - val_loss: 1.0049 - val_acc: 0.6780\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.9595 - acc: 0.7111 - val_loss: 0.9757 - val_acc: 0.6890\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.9294 - acc: 0.7200 - val_loss: 0.9498 - val_acc: 0.6920\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.9016 - acc: 0.7233 - val_loss: 0.9280 - val_acc: 0.6930\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.8761 - acc: 0.7296 - val_loss: 0.9067 - val_acc: 0.7010\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.8529 - acc: 0.7348 - val_loss: 0.8877 - val_acc: 0.6970\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.8314 - acc: 0.7397 - val_loss: 0.8729 - val_acc: 0.7050\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.8119 - acc: 0.7441 - val_loss: 0.8597 - val_acc: 0.7020\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.7928 - acc: 0.7479 - val_loss: 0.8415 - val_acc: 0.7130\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.7753 - acc: 0.7525 - val_loss: 0.8308 - val_acc: 0.7160\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.7596 - acc: 0.7556 - val_loss: 0.8190 - val_acc: 0.7170\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 0.7441 - acc: 0.7589 - val_loss: 0.8069 - val_acc: 0.7170\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 0.7300 - acc: 0.7637 - val_loss: 0.7964 - val_acc: 0.7190\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.7163 - acc: 0.7660 - val_loss: 0.7909 - val_acc: 0.7210\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.7039 - acc: 0.7695 - val_loss: 0.7795 - val_acc: 0.7220\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.6918 - acc: 0.7729 - val_loss: 0.7749 - val_acc: 0.7190\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6805 - acc: 0.7741 - val_loss: 0.7662 - val_acc: 0.7280\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.6698 - acc: 0.7785 - val_loss: 0.7621 - val_acc: 0.7290\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.6593 - acc: 0.7820 - val_loss: 0.7527 - val_acc: 0.7280\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 0.6494 - acc: 0.7828 - val_loss: 0.7478 - val_acc: 0.7360\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.6400 - acc: 0.7872 - val_loss: 0.7449 - val_acc: 0.7300\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.6314 - acc: 0.7876 - val_loss: 0.7389 - val_acc: 0.7360\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6224 - acc: 0.7932 - val_loss: 0.7331 - val_acc: 0.7290\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 0.6142 - acc: 0.7935 - val_loss: 0.7280 - val_acc: 0.7310\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6067 - acc: 0.7952 - val_loss: 0.7248 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.5991 - acc: 0.7977 - val_loss: 0.7205 - val_acc: 0.7270\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.5918 - acc: 0.7996 - val_loss: 0.7170 - val_acc: 0.7380\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.5844 - acc: 0.8027 - val_loss: 0.7160 - val_acc: 0.7390\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 0.5777 - acc: 0.8060 - val_loss: 0.7126 - val_acc: 0.7380\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.5710 - acc: 0.8069 - val_loss: 0.7109 - val_acc: 0.7370\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.5648 - acc: 0.8083 - val_loss: 0.7077 - val_acc: 0.7390\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.5585 - acc: 0.8116 - val_loss: 0.7079 - val_acc: 0.7350\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.5522 - acc: 0.8116 - val_loss: 0.7066 - val_acc: 0.7380\n",
      "Epoch 61/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.5464 - acc: 0.8137 - val_loss: 0.7009 - val_acc: 0.7410\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.5404 - acc: 0.8171 - val_loss: 0.6996 - val_acc: 0.7390\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.5350 - acc: 0.8185 - val_loss: 0.6986 - val_acc: 0.7370\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.5293 - acc: 0.8209 - val_loss: 0.7019 - val_acc: 0.7410\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 0.5235 - acc: 0.8213 - val_loss: 0.7000 - val_acc: 0.7360\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.5186 - acc: 0.8216 - val_loss: 0.6992 - val_acc: 0.7430\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 0.5135 - acc: 0.8257 - val_loss: 0.6909 - val_acc: 0.7410\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.5084 - acc: 0.8276 - val_loss: 0.6908 - val_acc: 0.7400\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.5036 - acc: 0.8275 - val_loss: 0.6908 - val_acc: 0.7450\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.4988 - acc: 0.8301 - val_loss: 0.6905 - val_acc: 0.7470\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.4937 - acc: 0.8339 - val_loss: 0.6879 - val_acc: 0.7410\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.4892 - acc: 0.8344 - val_loss: 0.6879 - val_acc: 0.7410\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 0.4851 - acc: 0.8357 - val_loss: 0.6877 - val_acc: 0.7480\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.4807 - acc: 0.8372 - val_loss: 0.6886 - val_acc: 0.7480\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.4756 - acc: 0.8383 - val_loss: 0.6890 - val_acc: 0.7400\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 0.4715 - acc: 0.8403 - val_loss: 0.6858 - val_acc: 0.7430\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.4671 - acc: 0.8443 - val_loss: 0.6838 - val_acc: 0.7400\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 0.4632 - acc: 0.8433 - val_loss: 0.6837 - val_acc: 0.7420\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 0.4591 - acc: 0.8443 - val_loss: 0.6829 - val_acc: 0.7430\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 0.4548 - acc: 0.8460 - val_loss: 0.6804 - val_acc: 0.7380\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.4509 - acc: 0.8475 - val_loss: 0.6824 - val_acc: 0.7450\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.4475 - acc: 0.8501 - val_loss: 0.6879 - val_acc: 0.7450\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.4434 - acc: 0.8500 - val_loss: 0.6852 - val_acc: 0.7450\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.4396 - acc: 0.8540 - val_loss: 0.6787 - val_acc: 0.7470\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.4361 - acc: 0.8533 - val_loss: 0.6809 - val_acc: 0.7450\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.4323 - acc: 0.8551 - val_loss: 0.6818 - val_acc: 0.7460\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.4285 - acc: 0.8571 - val_loss: 0.6808 - val_acc: 0.7460\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.4249 - acc: 0.8561 - val_loss: 0.6853 - val_acc: 0.7470\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.4213 - acc: 0.8593 - val_loss: 0.6861 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.4180 - acc: 0.8600 - val_loss: 0.6818 - val_acc: 0.7510\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.4145 - acc: 0.8620 - val_loss: 0.6823 - val_acc: 0.7500\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 0.4111 - acc: 0.8633 - val_loss: 0.6823 - val_acc: 0.7490\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.4075 - acc: 0.8651 - val_loss: 0.6816 - val_acc: 0.7480\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 0.4048 - acc: 0.8651 - val_loss: 0.6855 - val_acc: 0.7450\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.4010 - acc: 0.8681 - val_loss: 0.6877 - val_acc: 0.7500\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.3983 - acc: 0.8692 - val_loss: 0.6845 - val_acc: 0.7520\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.3947 - acc: 0.8708 - val_loss: 0.6816 - val_acc: 0.7510\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 106us/step - loss: 0.3911 - acc: 0.8727 - val_loss: 0.6883 - val_acc: 0.7490\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.3887 - acc: 0.8728 - val_loss: 0.6833 - val_acc: 0.7520\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.3855 - acc: 0.8736 - val_loss: 0.6846 - val_acc: 0.7510\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.3827 - acc: 0.8740 - val_loss: 0.6826 - val_acc: 0.7530\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.3795 - acc: 0.8763 - val_loss: 0.6862 - val_acc: 0.7560\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 0.3765 - acc: 0.8787 - val_loss: 0.6835 - val_acc: 0.7520\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 0.3736 - acc: 0.8788 - val_loss: 0.6890 - val_acc: 0.7530\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.3712 - acc: 0.8783 - val_loss: 0.6915 - val_acc: 0.7580\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.3681 - acc: 0.8808 - val_loss: 0.6891 - val_acc: 0.7580\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 0.3654 - acc: 0.8829 - val_loss: 0.6896 - val_acc: 0.7550\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 0.3626 - acc: 0.8843 - val_loss: 0.6900 - val_acc: 0.7550\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.3598 - acc: 0.8848 - val_loss: 0.6871 - val_acc: 0.7550\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 0.3569 - acc: 0.8868 - val_loss: 0.6937 - val_acc: 0.7570\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 0.3542 - acc: 0.8856 - val_loss: 0.6899 - val_acc: 0.7510\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.3517 - acc: 0.8888 - val_loss: 0.6909 - val_acc: 0.7540\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.3486 - acc: 0.8904 - val_loss: 0.6943 - val_acc: 0.7510\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.3465 - acc: 0.8905 - val_loss: 0.6954 - val_acc: 0.7560\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.3434 - acc: 0.8921 - val_loss: 0.6930 - val_acc: 0.7530\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.3407 - acc: 0.8935 - val_loss: 0.6946 - val_acc: 0.7570\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.3382 - acc: 0.8944 - val_loss: 0.6970 - val_acc: 0.7610\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.3366 - acc: 0.8955 - val_loss: 0.6926 - val_acc: 0.7570\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 0.3338 - acc: 0.8947 - val_loss: 0.6956 - val_acc: 0.7570\n",
      "Epoch 120/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.3312 - acc: 0.8957 - val_loss: 0.7016 - val_acc: 0.7520\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 3s 361us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 248us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3279268373966217, 0.8994666666348775]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.680192965666453, 0.7406666663487752]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'orange', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'm', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'aqua', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 3s 391us/step - loss: 1.9546 - acc: 0.1564 - val_loss: 1.9217 - val_acc: 0.1900\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.9287 - acc: 0.1796 - val_loss: 1.9042 - val_acc: 0.2070\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.9121 - acc: 0.1987 - val_loss: 1.8899 - val_acc: 0.2320\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.8964 - acc: 0.2140 - val_loss: 1.8750 - val_acc: 0.2490\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 1.8788 - acc: 0.2361 - val_loss: 1.8564 - val_acc: 0.2730\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 1.8565 - acc: 0.2668 - val_loss: 1.8313 - val_acc: 0.3130\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.8265 - acc: 0.3020 - val_loss: 1.7991 - val_acc: 0.3410\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.7892 - acc: 0.3355 - val_loss: 1.7593 - val_acc: 0.3730\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.7461 - acc: 0.3628 - val_loss: 1.7154 - val_acc: 0.3900\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.6990 - acc: 0.3845 - val_loss: 1.6701 - val_acc: 0.4100\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 1.6501 - acc: 0.4105 - val_loss: 1.6245 - val_acc: 0.4320\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.6002 - acc: 0.4356 - val_loss: 1.5760 - val_acc: 0.4560\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.5500 - acc: 0.4645 - val_loss: 1.5291 - val_acc: 0.4800\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.5004 - acc: 0.4916 - val_loss: 1.4814 - val_acc: 0.5040\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 1.4514 - acc: 0.5177 - val_loss: 1.4318 - val_acc: 0.5390\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.4040 - acc: 0.5437 - val_loss: 1.3894 - val_acc: 0.5460\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 1.3577 - acc: 0.5685 - val_loss: 1.3455 - val_acc: 0.5540\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 1.3139 - acc: 0.5835 - val_loss: 1.3097 - val_acc: 0.5780\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.2718 - acc: 0.6003 - val_loss: 1.2653 - val_acc: 0.5820\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.2319 - acc: 0.6124 - val_loss: 1.2315 - val_acc: 0.6020\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 1.1936 - acc: 0.6283 - val_loss: 1.1949 - val_acc: 0.6110\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 1s 194us/step - loss: 1.1570 - acc: 0.6357 - val_loss: 1.1650 - val_acc: 0.6210\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 1.1226 - acc: 0.6485 - val_loss: 1.1353 - val_acc: 0.6250\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 1.0900 - acc: 0.6588 - val_loss: 1.1071 - val_acc: 0.6390\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 1.0595 - acc: 0.6673 - val_loss: 1.0801 - val_acc: 0.6450\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.0300 - acc: 0.6761 - val_loss: 1.0547 - val_acc: 0.6490\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.0026 - acc: 0.6848 - val_loss: 1.0340 - val_acc: 0.6610\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.9763 - acc: 0.6937 - val_loss: 1.0112 - val_acc: 0.6570\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.9521 - acc: 0.6979 - val_loss: 0.9885 - val_acc: 0.6680\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.9283 - acc: 0.7055 - val_loss: 0.9723 - val_acc: 0.6730\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.9062 - acc: 0.7119 - val_loss: 0.9554 - val_acc: 0.6790\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.8847 - acc: 0.7201 - val_loss: 0.9403 - val_acc: 0.6770\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.8651 - acc: 0.7245 - val_loss: 0.9210 - val_acc: 0.6770\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.8455 - acc: 0.7311 - val_loss: 0.9069 - val_acc: 0.6840\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 0.8274 - acc: 0.7331 - val_loss: 0.8892 - val_acc: 0.6890\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.8103 - acc: 0.7389 - val_loss: 0.8793 - val_acc: 0.6900\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 0.7934 - acc: 0.7455 - val_loss: 0.8670 - val_acc: 0.6980\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.7770 - acc: 0.7500 - val_loss: 0.8538 - val_acc: 0.7030\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.7624 - acc: 0.7520 - val_loss: 0.8481 - val_acc: 0.6950\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 0.7479 - acc: 0.7565 - val_loss: 0.8358 - val_acc: 0.6980\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 0.7342 - acc: 0.7631 - val_loss: 0.8253 - val_acc: 0.7050\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 0.7211 - acc: 0.7631 - val_loss: 0.8153 - val_acc: 0.7070\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 0.7083 - acc: 0.7701 - val_loss: 0.8065 - val_acc: 0.7090\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 0.6960 - acc: 0.7684 - val_loss: 0.8021 - val_acc: 0.7170\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.6848 - acc: 0.7764 - val_loss: 0.7908 - val_acc: 0.7160\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 0.6740 - acc: 0.7772 - val_loss: 0.7832 - val_acc: 0.7190\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 0.6633 - acc: 0.7788 - val_loss: 0.7774 - val_acc: 0.7240\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.6536 - acc: 0.7823 - val_loss: 0.7775 - val_acc: 0.7240\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 0.6441 - acc: 0.7848 - val_loss: 0.7670 - val_acc: 0.7270\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6344 - acc: 0.7864 - val_loss: 0.7647 - val_acc: 0.7270\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 0.6258 - acc: 0.7880 - val_loss: 0.7582 - val_acc: 0.7270\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 0.6173 - acc: 0.7913 - val_loss: 0.7546 - val_acc: 0.7310\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6092 - acc: 0.7935 - val_loss: 0.7468 - val_acc: 0.7290\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 0.6010 - acc: 0.7935 - val_loss: 0.7412 - val_acc: 0.7350\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.5935 - acc: 0.7964 - val_loss: 0.7413 - val_acc: 0.7280\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 0.5866 - acc: 0.7975 - val_loss: 0.7381 - val_acc: 0.7330\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.5797 - acc: 0.8003 - val_loss: 0.7315 - val_acc: 0.7340\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 0.5724 - acc: 0.7997 - val_loss: 0.7260 - val_acc: 0.7340\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 0.5660 - acc: 0.8013 - val_loss: 0.7279 - val_acc: 0.7350\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 152us/step - loss: 0.5591 - acc: 0.8060 - val_loss: 0.7218 - val_acc: 0.7400\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 2s 305us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 422us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5555345640500386, 0.8056000000317891]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7103381279309591, 0.7226666661898296]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 235us/step - loss: 2.6173 - acc: 0.1277 - val_loss: 2.6019 - val_acc: 0.1360\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 1s 178us/step - loss: 2.5821 - acc: 0.1641 - val_loss: 2.5759 - val_acc: 0.1830\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 2.5568 - acc: 0.1992 - val_loss: 2.5537 - val_acc: 0.2190\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 2.5327 - acc: 0.2317 - val_loss: 2.5306 - val_acc: 0.2520\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 2.5068 - acc: 0.2657 - val_loss: 2.5049 - val_acc: 0.2750\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 2.4780 - acc: 0.3001 - val_loss: 2.4751 - val_acc: 0.3080\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 2.4457 - acc: 0.3379 - val_loss: 2.4421 - val_acc: 0.3330\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 2.4096 - acc: 0.3744 - val_loss: 2.4041 - val_acc: 0.3660\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 2.3701 - acc: 0.4003 - val_loss: 2.3638 - val_acc: 0.3970\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 2.3264 - acc: 0.4247 - val_loss: 2.3191 - val_acc: 0.4210\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 2.2791 - acc: 0.4535 - val_loss: 2.2708 - val_acc: 0.4430\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 2.2287 - acc: 0.4775 - val_loss: 2.2225 - val_acc: 0.4700\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 2.1757 - acc: 0.5073 - val_loss: 2.1693 - val_acc: 0.4890\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 2.1212 - acc: 0.5271 - val_loss: 2.1157 - val_acc: 0.5050\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 2.0663 - acc: 0.5464 - val_loss: 2.0615 - val_acc: 0.5420\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 2.0115 - acc: 0.5761 - val_loss: 2.0082 - val_acc: 0.5600\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 1.9573 - acc: 0.5947 - val_loss: 1.9560 - val_acc: 0.5720\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.9048 - acc: 0.6124 - val_loss: 1.9080 - val_acc: 0.5890\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.8543 - acc: 0.6357 - val_loss: 1.8614 - val_acc: 0.6050\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.8066 - acc: 0.6503 - val_loss: 1.8199 - val_acc: 0.6100\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.7619 - acc: 0.6624 - val_loss: 1.7782 - val_acc: 0.6320\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 1.7196 - acc: 0.6740 - val_loss: 1.7423 - val_acc: 0.6390\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.6803 - acc: 0.6856 - val_loss: 1.7078 - val_acc: 0.6380\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.6437 - acc: 0.6939 - val_loss: 1.6753 - val_acc: 0.6460\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.6097 - acc: 0.7036 - val_loss: 1.6442 - val_acc: 0.6630\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 1.5777 - acc: 0.7103 - val_loss: 1.6172 - val_acc: 0.6620\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.5484 - acc: 0.7128 - val_loss: 1.5903 - val_acc: 0.6650\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.5209 - acc: 0.7183 - val_loss: 1.5707 - val_acc: 0.6750\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.4957 - acc: 0.7224 - val_loss: 1.5490 - val_acc: 0.6750\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.4722 - acc: 0.7271 - val_loss: 1.5286 - val_acc: 0.6740\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.4497 - acc: 0.7309 - val_loss: 1.5108 - val_acc: 0.6810\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.4295 - acc: 0.7369 - val_loss: 1.4951 - val_acc: 0.6870\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.4095 - acc: 0.7437 - val_loss: 1.4788 - val_acc: 0.6910\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.3922 - acc: 0.7447 - val_loss: 1.4681 - val_acc: 0.6870\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.3746 - acc: 0.7492 - val_loss: 1.4579 - val_acc: 0.6880\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.3595 - acc: 0.7527 - val_loss: 1.4395 - val_acc: 0.6930\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.3440 - acc: 0.7565 - val_loss: 1.4288 - val_acc: 0.7010\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.3295 - acc: 0.7636 - val_loss: 1.4185 - val_acc: 0.7020\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.3159 - acc: 0.7677 - val_loss: 1.4126 - val_acc: 0.7030\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 1.3030 - acc: 0.7696 - val_loss: 1.3970 - val_acc: 0.7030\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.2907 - acc: 0.7707 - val_loss: 1.3923 - val_acc: 0.7050\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.2789 - acc: 0.7748 - val_loss: 1.3871 - val_acc: 0.7060\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 1.2672 - acc: 0.7787 - val_loss: 1.3745 - val_acc: 0.7130\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.2564 - acc: 0.7787 - val_loss: 1.3638 - val_acc: 0.7120\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.2457 - acc: 0.7831 - val_loss: 1.3585 - val_acc: 0.7150\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 1.2356 - acc: 0.7851 - val_loss: 1.3527 - val_acc: 0.7180\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.2258 - acc: 0.7888 - val_loss: 1.3470 - val_acc: 0.7180\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 1.2163 - acc: 0.7919 - val_loss: 1.3400 - val_acc: 0.7180\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.2071 - acc: 0.7959 - val_loss: 1.3318 - val_acc: 0.7180\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.1976 - acc: 0.7979 - val_loss: 1.3270 - val_acc: 0.7170\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.1889 - acc: 0.8003 - val_loss: 1.3234 - val_acc: 0.7210\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 2s 210us/step - loss: 1.1805 - acc: 0.8008 - val_loss: 1.3174 - val_acc: 0.7170\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.1726 - acc: 0.8027 - val_loss: 1.3133 - val_acc: 0.7230\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 1s 193us/step - loss: 1.1638 - acc: 0.8041 - val_loss: 1.3067 - val_acc: 0.7190\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 1.1565 - acc: 0.8073 - val_loss: 1.3018 - val_acc: 0.7250\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 1.1478 - acc: 0.8088 - val_loss: 1.2991 - val_acc: 0.7270\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.1406 - acc: 0.8096 - val_loss: 1.2920 - val_acc: 0.7260\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.1332 - acc: 0.8149 - val_loss: 1.2907 - val_acc: 0.7240\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 1.1259 - acc: 0.8133 - val_loss: 1.2837 - val_acc: 0.7260\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.1192 - acc: 0.8165 - val_loss: 1.2831 - val_acc: 0.7280\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.1117 - acc: 0.8193 - val_loss: 1.2767 - val_acc: 0.7260\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.1051 - acc: 0.8197 - val_loss: 1.2733 - val_acc: 0.7280\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.0984 - acc: 0.8220 - val_loss: 1.2697 - val_acc: 0.7240\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 1.0920 - acc: 0.8237 - val_loss: 1.2670 - val_acc: 0.7290\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.0851 - acc: 0.8253 - val_loss: 1.2600 - val_acc: 0.7310\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 1.0794 - acc: 0.8271 - val_loss: 1.2571 - val_acc: 0.7300\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 1.0721 - acc: 0.8291 - val_loss: 1.2567 - val_acc: 0.7320\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 1.0661 - acc: 0.8311 - val_loss: 1.2519 - val_acc: 0.7280\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.0599 - acc: 0.8297 - val_loss: 1.2486 - val_acc: 0.7320\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.0541 - acc: 0.8340 - val_loss: 1.2454 - val_acc: 0.7330\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.0485 - acc: 0.8355 - val_loss: 1.2414 - val_acc: 0.7300\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.0418 - acc: 0.8380 - val_loss: 1.2369 - val_acc: 0.7290\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 1.0368 - acc: 0.8379 - val_loss: 1.2367 - val_acc: 0.7290\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.0313 - acc: 0.8393 - val_loss: 1.2317 - val_acc: 0.7330\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.0250 - acc: 0.8405 - val_loss: 1.2351 - val_acc: 0.7350\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 120us/step - loss: 1.0203 - acc: 0.8409 - val_loss: 1.2253 - val_acc: 0.7370\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 1.0146 - acc: 0.8427 - val_loss: 1.2261 - val_acc: 0.7340\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.0090 - acc: 0.8439 - val_loss: 1.2220 - val_acc: 0.7350\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 1.0045 - acc: 0.8456 - val_loss: 1.2216 - val_acc: 0.7320\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.9986 - acc: 0.8484 - val_loss: 1.2155 - val_acc: 0.7320\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.9935 - acc: 0.8500 - val_loss: 1.2125 - val_acc: 0.7320\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.9883 - acc: 0.8509 - val_loss: 1.2106 - val_acc: 0.7340\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.9830 - acc: 0.8540 - val_loss: 1.2099 - val_acc: 0.7370\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.9780 - acc: 0.8536 - val_loss: 1.2039 - val_acc: 0.7400\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 0.9734 - acc: 0.8541 - val_loss: 1.2060 - val_acc: 0.7350\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.9682 - acc: 0.8555 - val_loss: 1.1998 - val_acc: 0.7350\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 0.9633 - acc: 0.8583 - val_loss: 1.2031 - val_acc: 0.7370\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 0.9585 - acc: 0.8556 - val_loss: 1.1959 - val_acc: 0.7370\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 0.9542 - acc: 0.8597 - val_loss: 1.1975 - val_acc: 0.7340\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.9493 - acc: 0.8599 - val_loss: 1.1946 - val_acc: 0.7350\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 0.9446 - acc: 0.8623 - val_loss: 1.1931 - val_acc: 0.7410\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.9403 - acc: 0.8639 - val_loss: 1.1914 - val_acc: 0.7330\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 0.9357 - acc: 0.8624 - val_loss: 1.1848 - val_acc: 0.7400\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 0.9312 - acc: 0.8660 - val_loss: 1.1827 - val_acc: 0.7360\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 0.9267 - acc: 0.8668 - val_loss: 1.1848 - val_acc: 0.7410\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 0.9222 - acc: 0.8656 - val_loss: 1.1773 - val_acc: 0.7450\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 0.9183 - acc: 0.8695 - val_loss: 1.1803 - val_acc: 0.7370\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 0.9134 - acc: 0.8699 - val_loss: 1.1783 - val_acc: 0.7460\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 0.9092 - acc: 0.8696 - val_loss: 1.1719 - val_acc: 0.7360\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.9055 - acc: 0.8704 - val_loss: 1.1762 - val_acc: 0.7340\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.9014 - acc: 0.8729 - val_loss: 1.1691 - val_acc: 0.7410\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 115us/step - loss: 0.8970 - acc: 0.8752 - val_loss: 1.1692 - val_acc: 0.7370\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.8929 - acc: 0.8735 - val_loss: 1.1669 - val_acc: 0.7430\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 0.8888 - acc: 0.8753 - val_loss: 1.1672 - val_acc: 0.7400\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 0.8850 - acc: 0.8779 - val_loss: 1.1644 - val_acc: 0.7410\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 0.8808 - acc: 0.8780 - val_loss: 1.1690 - val_acc: 0.7430\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.8771 - acc: 0.8757 - val_loss: 1.1623 - val_acc: 0.7390\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 0.8731 - acc: 0.8789 - val_loss: 1.1589 - val_acc: 0.7440\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.8693 - acc: 0.8773 - val_loss: 1.1606 - val_acc: 0.7380\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 0.8656 - acc: 0.8799 - val_loss: 1.1550 - val_acc: 0.7440\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.8621 - acc: 0.8823 - val_loss: 1.1530 - val_acc: 0.7470\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 0.8577 - acc: 0.8812 - val_loss: 1.1534 - val_acc: 0.7420\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 0.8543 - acc: 0.8815 - val_loss: 1.1508 - val_acc: 0.7450\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 0.8508 - acc: 0.8825 - val_loss: 1.1512 - val_acc: 0.7430\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 0.8474 - acc: 0.8841 - val_loss: 1.1489 - val_acc: 0.7440\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 2s 241us/step - loss: 0.8437 - acc: 0.8828 - val_loss: 1.1515 - val_acc: 0.7410s - loss: 0.8425 - acc: 0\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8402 - acc: 0.886 - 1s 137us/step - loss: 0.8401 - acc: 0.8860 - val_loss: 1.1502 - val_acc: 0.7400\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.8363 - acc: 0.8859 - val_loss: 1.1462 - val_acc: 0.7450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 1s 108us/step - loss: 0.8327 - acc: 0.8868 - val_loss: 1.1438 - val_acc: 0.7430\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 0.8292 - acc: 0.8875 - val_loss: 1.1405 - val_acc: 0.7420\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 3s 366us/step - loss: 16.0132 - acc: 0.1727 - val_loss: 15.5938 - val_acc: 0.2200\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 15.2398 - acc: 0.2144 - val_loss: 14.8392 - val_acc: 0.2400\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 1s 151us/step - loss: 14.4960 - acc: 0.2308 - val_loss: 14.1066 - val_acc: 0.2530\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 1s 175us/step - loss: 13.7714 - acc: 0.2527 - val_loss: 13.3911 - val_acc: 0.2720\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 13.0641 - acc: 0.2748 - val_loss: 12.6945 - val_acc: 0.2970\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 12.3751 - acc: 0.3140 - val_loss: 12.0144 - val_acc: 0.3260\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 11.7055 - acc: 0.3463 - val_loss: 11.3541 - val_acc: 0.3740\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 11.0554 - acc: 0.3817 - val_loss: 10.7143 - val_acc: 0.4070\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 10.4263 - acc: 0.4063 - val_loss: 10.0975 - val_acc: 0.4380\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 9.8204 - acc: 0.4388 - val_loss: 9.5049 - val_acc: 0.4640\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 9.2376 - acc: 0.4643 - val_loss: 8.9341 - val_acc: 0.4800\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 8.6787 - acc: 0.4904 - val_loss: 8.3891 - val_acc: 0.4950\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 8.1434 - acc: 0.5057 - val_loss: 7.8689 - val_acc: 0.5220\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 7.6318 - acc: 0.5271 - val_loss: 7.3726 - val_acc: 0.5470\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 7.1447 - acc: 0.5451 - val_loss: 6.8962 - val_acc: 0.5560\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 6.6809 - acc: 0.5613 - val_loss: 6.4462 - val_acc: 0.5720\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 6.2411 - acc: 0.5783 - val_loss: 6.0215 - val_acc: 0.5760\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 5.8248 - acc: 0.5908 - val_loss: 5.6181 - val_acc: 0.5930\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 1s 139us/step - loss: 5.4313 - acc: 0.6076 - val_loss: 5.2388 - val_acc: 0.5970\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 5.0610 - acc: 0.6175 - val_loss: 4.8821 - val_acc: 0.6120\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 4.7128 - acc: 0.6252 - val_loss: 4.5447 - val_acc: 0.6180\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 4.3868 - acc: 0.6316 - val_loss: 4.2339 - val_acc: 0.6180\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 4.0837 - acc: 0.6389 - val_loss: 3.9414 - val_acc: 0.6200\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 2s 202us/step - loss: 3.8029 - acc: 0.6448 - val_loss: 3.6722 - val_acc: 0.6290\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 1s 145us/step - loss: 3.5439 - acc: 0.6481 - val_loss: 3.4253 - val_acc: 0.6360\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 3.3073 - acc: 0.6543 - val_loss: 3.2020 - val_acc: 0.6320\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 3.0926 - acc: 0.6544 - val_loss: 2.9978 - val_acc: 0.6500\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 2.8986 - acc: 0.6593 - val_loss: 2.8177 - val_acc: 0.6410\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 2.7262 - acc: 0.6613 - val_loss: 2.6557 - val_acc: 0.6400\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 2.5743 - acc: 0.6653 - val_loss: 2.5129 - val_acc: 0.6510\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 2.4434 - acc: 0.6672 - val_loss: 2.3943 - val_acc: 0.6460\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 2.3324 - acc: 0.6677 - val_loss: 2.2962 - val_acc: 0.6430\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 2.2411 - acc: 0.6700 - val_loss: 2.2111 - val_acc: 0.6510\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 2.1675 - acc: 0.6716 - val_loss: 2.1467 - val_acc: 0.6470\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 2.1105 - acc: 0.6728 - val_loss: 2.0969 - val_acc: 0.6430\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 2.0686 - acc: 0.6728 - val_loss: 2.0637 - val_acc: 0.6560\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 2.0370 - acc: 0.6767 - val_loss: 2.0339 - val_acc: 0.6570\n",
      "Epoch 38/120\n",
      "1536/7500 [=====>........................] - ETA: 1s - loss: 2.0384 - acc: 0.6712"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenylmurdock/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.105578). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 179us/step - loss: 2.0114 - acc: 0.6771 - val_loss: 2.0136 - val_acc: 0.6520\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 1s 146us/step - loss: 1.9894 - acc: 0.6772 - val_loss: 1.9928 - val_acc: 0.6580\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 1s 153us/step - loss: 1.9694 - acc: 0.6811 - val_loss: 1.9726 - val_acc: 0.6520\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.9505 - acc: 0.6816 - val_loss: 1.9548 - val_acc: 0.6540\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 1.9327 - acc: 0.6823 - val_loss: 1.9371 - val_acc: 0.6540\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 1.9160 - acc: 0.6864 - val_loss: 1.9250 - val_acc: 0.6580\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.8997 - acc: 0.6877 - val_loss: 1.9082 - val_acc: 0.6580\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 1.8848 - acc: 0.6903 - val_loss: 1.8945 - val_acc: 0.6580\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.8698 - acc: 0.6944 - val_loss: 1.8802 - val_acc: 0.6660\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 1.8556 - acc: 0.6927 - val_loss: 1.8642 - val_acc: 0.6560\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.8419 - acc: 0.6900 - val_loss: 1.8540 - val_acc: 0.6600\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 1s 132us/step - loss: 1.8286 - acc: 0.6944 - val_loss: 1.8422 - val_acc: 0.6650\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 1s 169us/step - loss: 1.8156 - acc: 0.6967 - val_loss: 1.8278 - val_acc: 0.6700\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 1.8032 - acc: 0.6977 - val_loss: 1.8191 - val_acc: 0.6660\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 2s 201us/step - loss: 1.7909 - acc: 0.6988 - val_loss: 1.8043 - val_acc: 0.6710\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.7795 - acc: 0.6995 - val_loss: 1.7974 - val_acc: 0.6750\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.7675 - acc: 0.700 - 1s 174us/step - loss: 1.7678 - acc: 0.7000 - val_loss: 1.7836 - val_acc: 0.6770\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 1.7566 - acc: 0.6995 - val_loss: 1.7700 - val_acc: 0.6760\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.7454 - acc: 0.6997 - val_loss: 1.7604 - val_acc: 0.6730\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.7350 - acc: 0.7028 - val_loss: 1.7553 - val_acc: 0.6730\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.7245 - acc: 0.7024 - val_loss: 1.7415 - val_acc: 0.6760\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 1.7141 - acc: 0.7024 - val_loss: 1.7317 - val_acc: 0.6760\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.7043 - acc: 0.7021 - val_loss: 1.7242 - val_acc: 0.6750\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.6944 - acc: 0.7047 - val_loss: 1.7155 - val_acc: 0.6810\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.6849 - acc: 0.7049 - val_loss: 1.7065 - val_acc: 0.6740\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.6761 - acc: 0.7029 - val_loss: 1.7023 - val_acc: 0.6790\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.6664 - acc: 0.7040 - val_loss: 1.6873 - val_acc: 0.6800\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.6574 - acc: 0.7068 - val_loss: 1.6772 - val_acc: 0.6840\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 1.6481 - acc: 0.7061 - val_loss: 1.6749 - val_acc: 0.6870\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 1.6396 - acc: 0.7061 - val_loss: 1.6605 - val_acc: 0.6830\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 1.6306 - acc: 0.7077 - val_loss: 1.6570 - val_acc: 0.6840\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 1s 123us/step - loss: 1.6227 - acc: 0.7068 - val_loss: 1.6509 - val_acc: 0.6890\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 1s 141us/step - loss: 1.6151 - acc: 0.7073 - val_loss: 1.6386 - val_acc: 0.6870\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 1s 143us/step - loss: 1.6067 - acc: 0.7097 - val_loss: 1.6306 - val_acc: 0.6880\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 1.5982 - acc: 0.7081 - val_loss: 1.6231 - val_acc: 0.6910\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 1s 144us/step - loss: 1.5905 - acc: 0.7096 - val_loss: 1.6173 - val_acc: 0.6890\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 1.5831 - acc: 0.7089 - val_loss: 1.6070 - val_acc: 0.6910\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 1.5760 - acc: 0.7109 - val_loss: 1.6003 - val_acc: 0.6940\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.5678 - acc: 0.7111 - val_loss: 1.5972 - val_acc: 0.6900\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.5606 - acc: 0.7104 - val_loss: 1.5910 - val_acc: 0.6950\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 150us/step - loss: 1.5539 - acc: 0.7107 - val_loss: 1.5801 - val_acc: 0.6860\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 140us/step - loss: 1.5463 - acc: 0.7113 - val_loss: 1.5726 - val_acc: 0.6950\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 1.5394 - acc: 0.7105 - val_loss: 1.5760 - val_acc: 0.6830\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.5326 - acc: 0.7140 - val_loss: 1.5619 - val_acc: 0.6980\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 1.5262 - acc: 0.7115 - val_loss: 1.5533 - val_acc: 0.6900\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 1s 126us/step - loss: 1.5189 - acc: 0.7137 - val_loss: 1.5488 - val_acc: 0.6890\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 1s 122us/step - loss: 1.5128 - acc: 0.7117 - val_loss: 1.5420 - val_acc: 0.6920\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 1s 134us/step - loss: 1.5062 - acc: 0.7133 - val_loss: 1.5341 - val_acc: 0.6940\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 1s 128us/step - loss: 1.4990 - acc: 0.7125 - val_loss: 1.5292 - val_acc: 0.6960\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.4928 - acc: 0.7148 - val_loss: 1.5240 - val_acc: 0.6920\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 1s 111us/step - loss: 1.4862 - acc: 0.7153 - val_loss: 1.5182 - val_acc: 0.6910\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 1s 138us/step - loss: 1.4800 - acc: 0.7147 - val_loss: 1.5116 - val_acc: 0.6950\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.4741 - acc: 0.7164 - val_loss: 1.5037 - val_acc: 0.7000\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.4675 - acc: 0.7163 - val_loss: 1.5071 - val_acc: 0.6850\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.4617 - acc: 0.7149 - val_loss: 1.4942 - val_acc: 0.6900\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 1s 137us/step - loss: 1.4557 - acc: 0.7168 - val_loss: 1.4890 - val_acc: 0.6910\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 1s 136us/step - loss: 1.4498 - acc: 0.7176 - val_loss: 1.4834 - val_acc: 0.6900\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 1s 127us/step - loss: 1.4438 - acc: 0.7172 - val_loss: 1.4822 - val_acc: 0.6950\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 1.4375 - acc: 0.7193 - val_loss: 1.4691 - val_acc: 0.6960\n",
      "Epoch 97/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.4322 - acc: 0.7169 - val_loss: 1.4660 - val_acc: 0.7020\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 1.4259 - acc: 0.7169 - val_loss: 1.4576 - val_acc: 0.6990\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 1s 197us/step - loss: 1.4198 - acc: 0.7187 - val_loss: 1.4535 - val_acc: 0.7000\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 1.4150 - acc: 0.7183 - val_loss: 1.4499 - val_acc: 0.6930\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 133us/step - loss: 1.4093 - acc: 0.7176 - val_loss: 1.4462 - val_acc: 0.7000\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.4041 - acc: 0.7201 - val_loss: 1.4421 - val_acc: 0.7010\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.3980 - acc: 0.7203 - val_loss: 1.4316 - val_acc: 0.7010\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 1s 107us/step - loss: 1.3929 - acc: 0.7201 - val_loss: 1.4295 - val_acc: 0.6980\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.3875 - acc: 0.7221 - val_loss: 1.4297 - val_acc: 0.6980\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.3824 - acc: 0.7197 - val_loss: 1.4202 - val_acc: 0.6980\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3767 - acc: 0.7223 - val_loss: 1.4135 - val_acc: 0.7000\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.3714 - acc: 0.7229 - val_loss: 1.4105 - val_acc: 0.7000\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 1s 109us/step - loss: 1.3667 - acc: 0.7236 - val_loss: 1.4017 - val_acc: 0.7010\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 104us/step - loss: 1.3613 - acc: 0.7243 - val_loss: 1.4031 - val_acc: 0.6940\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.3563 - acc: 0.7244 - val_loss: 1.3940 - val_acc: 0.7050\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 1.3514 - acc: 0.7245 - val_loss: 1.3903 - val_acc: 0.7010\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.3461 - acc: 0.7263 - val_loss: 1.3862 - val_acc: 0.7030\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 1s 112us/step - loss: 1.3414 - acc: 0.7241 - val_loss: 1.3789 - val_acc: 0.7040\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.3371 - acc: 0.7269 - val_loss: 1.3750 - val_acc: 0.7010\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.3322 - acc: 0.7275 - val_loss: 1.3739 - val_acc: 0.7010\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 1.3276 - acc: 0.7271 - val_loss: 1.3666 - val_acc: 0.7010\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 1.3224 - acc: 0.7273 - val_loss: 1.3605 - val_acc: 0.7030\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 1.3178 - acc: 0.7283 - val_loss: 1.3584 - val_acc: 0.7040\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 1.3137 - acc: 0.7284 - val_loss: 1.3627 - val_acc: 0.6970\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'm.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0140 - acc: 0.1877 - val_loss: 15.6118 - val_acc: 0.2100\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 15.2533 - acc: 0.2067 - val_loss: 14.8668 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 14.5171 - acc: 0.2219 - val_loss: 14.1433 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.8011 - acc: 0.2360 - val_loss: 13.4388 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.1038 - acc: 0.2528 - val_loss: 12.7534 - val_acc: 0.2580\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 12.4253 - acc: 0.2772 - val_loss: 12.0869 - val_acc: 0.2720\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7658 - acc: 0.3027 - val_loss: 11.4387 - val_acc: 0.3140\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1259 - acc: 0.3431 - val_loss: 10.8103 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5062 - acc: 0.3781 - val_loss: 10.2028 - val_acc: 0.3780\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9068 - acc: 0.4124 - val_loss: 9.6137 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3279 - acc: 0.4441 - val_loss: 9.0463 - val_acc: 0.4310\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7708 - acc: 0.4668 - val_loss: 8.5007 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2357 - acc: 0.4971 - val_loss: 7.9774 - val_acc: 0.4730\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7237 - acc: 0.5213 - val_loss: 7.4776 - val_acc: 0.4990\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2345 - acc: 0.5405 - val_loss: 7.0011 - val_acc: 0.5270\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7686 - acc: 0.5620 - val_loss: 6.5474 - val_acc: 0.5490\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.3256 - acc: 0.5787 - val_loss: 6.1182 - val_acc: 0.5860\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9053 - acc: 0.5941 - val_loss: 5.7067 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5074 - acc: 0.6119 - val_loss: 5.3218 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.1324 - acc: 0.6183 - val_loss: 4.9565 - val_acc: 0.6200\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.7803 - acc: 0.6311 - val_loss: 4.6160 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4521 - acc: 0.6364 - val_loss: 4.2978 - val_acc: 0.6320\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.1454 - acc: 0.6468 - val_loss: 4.0029 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8615 - acc: 0.6531 - val_loss: 3.7284 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5996 - acc: 0.6593 - val_loss: 3.4779 - val_acc: 0.6590\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.3602 - acc: 0.6624 - val_loss: 3.2500 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1425 - acc: 0.6635 - val_loss: 3.0417 - val_acc: 0.6780\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9463 - acc: 0.6691 - val_loss: 2.8538 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.7710 - acc: 0.6711 - val_loss: 2.6896 - val_acc: 0.6810\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6167 - acc: 0.6717 - val_loss: 2.5434 - val_acc: 0.6810\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4825 - acc: 0.6703 - val_loss: 2.4194 - val_acc: 0.6810\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.3687 - acc: 0.6731 - val_loss: 2.3143 - val_acc: 0.6780\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2740 - acc: 0.6723 - val_loss: 2.2270 - val_acc: 0.6820\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1980 - acc: 0.6713 - val_loss: 2.1602 - val_acc: 0.6810\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1386 - acc: 0.6725 - val_loss: 2.1087 - val_acc: 0.6800\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0942 - acc: 0.6736 - val_loss: 2.0704 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0616 - acc: 0.6712 - val_loss: 2.0393 - val_acc: 0.6720\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0343 - acc: 0.6735 - val_loss: 2.0143 - val_acc: 0.6710\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0114 - acc: 0.6720 - val_loss: 1.9902 - val_acc: 0.6790\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9903 - acc: 0.6745 - val_loss: 1.9708 - val_acc: 0.6780\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9712 - acc: 0.6747 - val_loss: 1.9519 - val_acc: 0.6830\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9536 - acc: 0.6751 - val_loss: 1.9332 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9369 - acc: 0.6753 - val_loss: 1.9168 - val_acc: 0.6820\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9213 - acc: 0.6769 - val_loss: 1.9012 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9061 - acc: 0.6791 - val_loss: 1.8871 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8920 - acc: 0.6776 - val_loss: 1.8731 - val_acc: 0.6830\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8783 - acc: 0.6800 - val_loss: 1.8585 - val_acc: 0.6820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8650 - acc: 0.6796 - val_loss: 1.8430 - val_acc: 0.6850\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8519 - acc: 0.6815 - val_loss: 1.8301 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8395 - acc: 0.6824 - val_loss: 1.8186 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8277 - acc: 0.6844 - val_loss: 1.8058 - val_acc: 0.6900\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8157 - acc: 0.6853 - val_loss: 1.7949 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8043 - acc: 0.6844 - val_loss: 1.7821 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7931 - acc: 0.6857 - val_loss: 1.7722 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7828 - acc: 0.6863 - val_loss: 1.7612 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7722 - acc: 0.6876 - val_loss: 1.7494 - val_acc: 0.6940\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7616 - acc: 0.6877 - val_loss: 1.7391 - val_acc: 0.6920\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7517 - acc: 0.6887 - val_loss: 1.7376 - val_acc: 0.6940\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7419 - acc: 0.6892 - val_loss: 1.7217 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7319 - acc: 0.6887 - val_loss: 1.7170 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7227 - acc: 0.6892 - val_loss: 1.7054 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7136 - acc: 0.6903 - val_loss: 1.6913 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7041 - acc: 0.6924 - val_loss: 1.6814 - val_acc: 0.6940\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6951 - acc: 0.6928 - val_loss: 1.6735 - val_acc: 0.7040\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6864 - acc: 0.6944 - val_loss: 1.6635 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6778 - acc: 0.6957 - val_loss: 1.6551 - val_acc: 0.7050\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6691 - acc: 0.6960 - val_loss: 1.6475 - val_acc: 0.7070\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6604 - acc: 0.6987 - val_loss: 1.6402 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6518 - acc: 0.6996 - val_loss: 1.6361 - val_acc: 0.7010\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6436 - acc: 0.6987 - val_loss: 1.6220 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6354 - acc: 0.7015 - val_loss: 1.6156 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6274 - acc: 0.7015 - val_loss: 1.6048 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6198 - acc: 0.7019 - val_loss: 1.6047 - val_acc: 0.7080\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6122 - acc: 0.7016 - val_loss: 1.5891 - val_acc: 0.7060\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6044 - acc: 0.7020 - val_loss: 1.5813 - val_acc: 0.7070\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5962 - acc: 0.7048 - val_loss: 1.5755 - val_acc: 0.7060\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5886 - acc: 0.7039 - val_loss: 1.5679 - val_acc: 0.7060\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5609 - val_acc: 0.7080\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5739 - acc: 0.7051 - val_loss: 1.5524 - val_acc: 0.7100\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5670 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5596 - acc: 0.7049 - val_loss: 1.5392 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.7080 - val_loss: 1.5329 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5455 - acc: 0.7063 - val_loss: 1.5254 - val_acc: 0.7150\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7087 - val_loss: 1.5181 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5314 - acc: 0.7088 - val_loss: 1.5132 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5249 - acc: 0.7084 - val_loss: 1.5042 - val_acc: 0.7140\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5176 - acc: 0.7093 - val_loss: 1.5022 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5115 - acc: 0.7108 - val_loss: 1.4923 - val_acc: 0.7130\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7117 - val_loss: 1.4951 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4983 - acc: 0.7115 - val_loss: 1.4801 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7127 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4852 - acc: 0.7104 - val_loss: 1.4667 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7136 - val_loss: 1.4594 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4724 - acc: 0.7137 - val_loss: 1.4551 - val_acc: 0.7140\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4658 - acc: 0.7149 - val_loss: 1.4498 - val_acc: 0.7160\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4602 - acc: 0.7151 - val_loss: 1.4421 - val_acc: 0.7170\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7163 - val_loss: 1.4346 - val_acc: 0.7170\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4470 - acc: 0.7149 - val_loss: 1.4340 - val_acc: 0.7200\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4414 - acc: 0.7152 - val_loss: 1.4272 - val_acc: 0.7180\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4362 - acc: 0.7188 - val_loss: 1.4224 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4301 - acc: 0.7153 - val_loss: 1.4136 - val_acc: 0.7190\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4246 - acc: 0.7169 - val_loss: 1.4046 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4182 - acc: 0.7196 - val_loss: 1.4068 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4132 - acc: 0.7200 - val_loss: 1.3998 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7176 - val_loss: 1.3891 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4016 - acc: 0.7204 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3966 - acc: 0.7208 - val_loss: 1.3825 - val_acc: 0.7210\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3906 - acc: 0.7225 - val_loss: 1.3804 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3854 - acc: 0.7219 - val_loss: 1.3692 - val_acc: 0.7220\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3802 - acc: 0.7215 - val_loss: 1.3651 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3750 - acc: 0.7201 - val_loss: 1.3593 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3703 - acc: 0.7241 - val_loss: 1.3563 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3650 - acc: 0.7241 - val_loss: 1.3489 - val_acc: 0.7250\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3596 - acc: 0.7249 - val_loss: 1.3460 - val_acc: 0.7250\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3543 - acc: 0.7255 - val_loss: 1.3375 - val_acc: 0.7250\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7248 - val_loss: 1.3340 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3450 - acc: 0.7249 - val_loss: 1.3339 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3404 - acc: 0.7247 - val_loss: 1.3267 - val_acc: 0.7210\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3356 - acc: 0.7267 - val_loss: 1.3237 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3309 - acc: 0.7299 - val_loss: 1.3153 - val_acc: 0.7290\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3262 - acc: 0.7273 - val_loss: 1.3085 - val_acc: 0.7270\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3214 - acc: 0.7275 - val_loss: 1.3051 - val_acc: 0.7210\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.7279 - val_loss: 1.3032 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3118 - acc: 0.7288 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3073 - acc: 0.7301 - val_loss: 1.2923 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3030 - acc: 0.7299 - val_loss: 1.2887 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2984 - acc: 0.7301 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2947 - acc: 0.7303 - val_loss: 1.2849 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2899 - acc: 0.7319 - val_loss: 1.2752 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2858 - acc: 0.7301 - val_loss: 1.2707 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2819 - acc: 0.7319 - val_loss: 1.2682 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2771 - acc: 0.7319 - val_loss: 1.2628 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2734 - acc: 0.7331 - val_loss: 1.2590 - val_acc: 0.7290\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2691 - acc: 0.7335 - val_loss: 1.2582 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2655 - acc: 0.7348 - val_loss: 1.2505 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2614 - acc: 0.7351 - val_loss: 1.2478 - val_acc: 0.7320\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2575 - acc: 0.7325 - val_loss: 1.2434 - val_acc: 0.7270\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2538 - acc: 0.7355 - val_loss: 1.2403 - val_acc: 0.7290\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2497 - acc: 0.7355 - val_loss: 1.2362 - val_acc: 0.7290\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2456 - acc: 0.7341 - val_loss: 1.2344 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2418 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2380 - acc: 0.7364 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2344 - acc: 0.7365 - val_loss: 1.2225 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2306 - acc: 0.7371 - val_loss: 1.2170 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2272 - acc: 0.7349 - val_loss: 1.2162 - val_acc: 0.7320\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2233 - acc: 0.7385 - val_loss: 1.2114 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2199 - acc: 0.7385 - val_loss: 1.2098 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2166 - acc: 0.7379 - val_loss: 1.2034 - val_acc: 0.7340\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2128 - acc: 0.7393 - val_loss: 1.2085 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2096 - acc: 0.7389 - val_loss: 1.1977 - val_acc: 0.7310\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2066 - acc: 0.7391 - val_loss: 1.1958 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2030 - acc: 0.7389 - val_loss: 1.1909 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1992 - acc: 0.7396 - val_loss: 1.1875 - val_acc: 0.7350\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1969 - acc: 0.7380 - val_loss: 1.1904 - val_acc: 0.7290\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1932 - acc: 0.7395 - val_loss: 1.1811 - val_acc: 0.7340\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1896 - acc: 0.7405 - val_loss: 1.1802 - val_acc: 0.7370\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1861 - acc: 0.7408 - val_loss: 1.1754 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1834 - acc: 0.7409 - val_loss: 1.1755 - val_acc: 0.7320\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1803 - acc: 0.7425 - val_loss: 1.1680 - val_acc: 0.7360\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1766 - acc: 0.7424 - val_loss: 1.1751 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1752 - acc: 0.7417 - val_loss: 1.1609 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1716 - acc: 0.7423 - val_loss: 1.1612 - val_acc: 0.7360\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1687 - acc: 0.7409 - val_loss: 1.1594 - val_acc: 0.7370\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1658 - acc: 0.7441 - val_loss: 1.1571 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1629 - acc: 0.7419 - val_loss: 1.1572 - val_acc: 0.7390\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1598 - acc: 0.7429 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7429 - val_loss: 1.1462 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1546 - acc: 0.7427 - val_loss: 1.1453 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1517 - acc: 0.7431 - val_loss: 1.1406 - val_acc: 0.7410\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1490 - acc: 0.7461 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1470 - acc: 0.7456 - val_loss: 1.1377 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1438 - acc: 0.7455 - val_loss: 1.1356 - val_acc: 0.7440\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1415 - acc: 0.7457 - val_loss: 1.1324 - val_acc: 0.7390\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1394 - acc: 0.7433 - val_loss: 1.1291 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1362 - acc: 0.7455 - val_loss: 1.1279 - val_acc: 0.7440\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7449 - val_loss: 1.1235 - val_acc: 0.7370\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1320 - acc: 0.7447 - val_loss: 1.1211 - val_acc: 0.7380\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1293 - acc: 0.7463 - val_loss: 1.1236 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1279 - acc: 0.7457 - val_loss: 1.1186 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1250 - acc: 0.7467 - val_loss: 1.1250 - val_acc: 0.7390\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1234 - acc: 0.7436 - val_loss: 1.1210 - val_acc: 0.7360\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1214 - acc: 0.7468 - val_loss: 1.1116 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1188 - acc: 0.7488 - val_loss: 1.1138 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1174 - acc: 0.7473 - val_loss: 1.1087 - val_acc: 0.7440\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1145 - acc: 0.7479 - val_loss: 1.1078 - val_acc: 0.7410\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1129 - acc: 0.7463 - val_loss: 1.1084 - val_acc: 0.7450\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1110 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1081 - acc: 0.7471 - val_loss: 1.1027 - val_acc: 0.7410\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1065 - acc: 0.7489 - val_loss: 1.0996 - val_acc: 0.7380\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1045 - acc: 0.7473 - val_loss: 1.0977 - val_acc: 0.7430\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1028 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7370\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1010 - acc: 0.7475 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0997 - acc: 0.7484 - val_loss: 1.0904 - val_acc: 0.7450\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0972 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0953 - acc: 0.7503 - val_loss: 1.0877 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0937 - acc: 0.7480 - val_loss: 1.0867 - val_acc: 0.7390\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0922 - acc: 0.7485 - val_loss: 1.0866 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0896 - acc: 0.7497 - val_loss: 1.0832 - val_acc: 0.7440\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0879 - acc: 0.7495 - val_loss: 1.0827 - val_acc: 0.7440\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7511 - val_loss: 1.0830 - val_acc: 0.7430\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.7515 - val_loss: 1.0847 - val_acc: 0.7440\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0835 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7420\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.7508 - val_loss: 1.0765 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0800 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7480\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0782 - acc: 0.7513 - val_loss: 1.0711 - val_acc: 0.7430\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0765 - acc: 0.7496 - val_loss: 1.0708 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0753 - acc: 0.7517 - val_loss: 1.0696 - val_acc: 0.7420\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0740 - acc: 0.7512 - val_loss: 1.0685 - val_acc: 0.7400\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.7521 - val_loss: 1.0647 - val_acc: 0.7430\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0696 - acc: 0.7523 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7537 - val_loss: 1.0667 - val_acc: 0.7420\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0681 - acc: 0.7524 - val_loss: 1.0655 - val_acc: 0.7390\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0656 - acc: 0.7537 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0650 - acc: 0.7543 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0624 - acc: 0.7532 - val_loss: 1.0579 - val_acc: 0.7480\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7535 - val_loss: 1.0581 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0599 - acc: 0.7533 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0584 - acc: 0.7539 - val_loss: 1.0534 - val_acc: 0.7440\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0572 - acc: 0.7547 - val_loss: 1.0521 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0560 - acc: 0.7552 - val_loss: 1.0489 - val_acc: 0.7490\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.7543 - val_loss: 1.0516 - val_acc: 0.7460\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.7552 - val_loss: 1.0571 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0526 - acc: 0.7544 - val_loss: 1.0492 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0509 - acc: 0.7527 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0493 - acc: 0.7540 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0479 - acc: 0.7555 - val_loss: 1.0413 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0465 - acc: 0.7545 - val_loss: 1.0415 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.7561 - val_loss: 1.0418 - val_acc: 0.7440\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0438 - acc: 0.7551 - val_loss: 1.0408 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.7563 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0410 - acc: 0.7564 - val_loss: 1.0371 - val_acc: 0.7440\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0400 - acc: 0.7569 - val_loss: 1.0419 - val_acc: 0.7370\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0390 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7450\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0372 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7400\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0359 - acc: 0.7575 - val_loss: 1.0348 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0334 - acc: 0.7564 - val_loss: 1.0296 - val_acc: 0.7510\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0328 - acc: 0.7564 - val_loss: 1.0358 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0323 - acc: 0.7555 - val_loss: 1.0284 - val_acc: 0.7460\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0306 - acc: 0.7579 - val_loss: 1.0279 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0292 - acc: 0.7588 - val_loss: 1.0257 - val_acc: 0.7470\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.7576 - val_loss: 1.0270 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0269 - acc: 0.7560 - val_loss: 1.0248 - val_acc: 0.7420\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0254 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0244 - acc: 0.7569 - val_loss: 1.0243 - val_acc: 0.7450\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0233 - acc: 0.7599 - val_loss: 1.0241 - val_acc: 0.7460\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0225 - acc: 0.7581 - val_loss: 1.0215 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0215 - acc: 0.7583 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0202 - acc: 0.7609 - val_loss: 1.0187 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0196 - acc: 0.7559 - val_loss: 1.0175 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0179 - acc: 0.7597 - val_loss: 1.0187 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0173 - acc: 0.7612 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0165 - acc: 0.7596 - val_loss: 1.0164 - val_acc: 0.7520\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0160 - acc: 0.7591 - val_loss: 1.0137 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0143 - acc: 0.7608 - val_loss: 1.0162 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7603 - val_loss: 1.0109 - val_acc: 0.7510\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0121 - acc: 0.7615 - val_loss: 1.0209 - val_acc: 0.7530\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0118 - acc: 0.7607 - val_loss: 1.0087 - val_acc: 0.7530\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0103 - acc: 0.7604 - val_loss: 1.0091 - val_acc: 0.7490\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0092 - acc: 0.7607 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.7613 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0073 - acc: 0.7589 - val_loss: 1.0057 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7624 - val_loss: 1.0074 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0054 - acc: 0.7617 - val_loss: 1.0050 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0038 - acc: 0.7629 - val_loss: 1.0060 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0037 - acc: 0.7595 - val_loss: 1.0063 - val_acc: 0.7470\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0023 - acc: 0.7624 - val_loss: 1.0027 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0012 - acc: 0.7624 - val_loss: 1.0099 - val_acc: 0.7470\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0015 - acc: 0.7605 - val_loss: 1.0015 - val_acc: 0.7520\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7640 - val_loss: 0.9987 - val_acc: 0.7490\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9989 - acc: 0.7629 - val_loss: 0.9976 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9978 - acc: 0.7636 - val_loss: 1.0029 - val_acc: 0.7480\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9971 - acc: 0.7621 - val_loss: 0.9973 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9967 - acc: 0.7612 - val_loss: 0.9938 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9952 - acc: 0.7636 - val_loss: 0.9967 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9945 - acc: 0.7635 - val_loss: 1.0000 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7629 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9926 - acc: 0.7643 - val_loss: 0.9954 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9927 - acc: 0.7635 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9908 - acc: 0.7643 - val_loss: 0.9973 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9906 - acc: 0.7633 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9895 - acc: 0.7609 - val_loss: 0.9884 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9884 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9876 - acc: 0.7648 - val_loss: 0.9932 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9868 - acc: 0.7620 - val_loss: 0.9950 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9861 - acc: 0.7667 - val_loss: 0.9908 - val_acc: 0.7520\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9855 - acc: 0.7635 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9847 - acc: 0.7668 - val_loss: 0.9927 - val_acc: 0.7400\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.7645 - val_loss: 0.9864 - val_acc: 0.7560\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9829 - acc: 0.7653 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9823 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9815 - acc: 0.7645 - val_loss: 0.9833 - val_acc: 0.7570\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9808 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9789 - acc: 0.7656 - val_loss: 0.9922 - val_acc: 0.7470\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9784 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9785 - acc: 0.7648 - val_loss: 0.9781 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9774 - acc: 0.7657 - val_loss: 0.9808 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9768 - acc: 0.7657 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.7667 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7656 - val_loss: 0.9809 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9749 - acc: 0.7655 - val_loss: 0.9872 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9742 - acc: 0.7675 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9733 - acc: 0.7692 - val_loss: 0.9749 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9723 - acc: 0.7675 - val_loss: 0.9831 - val_acc: 0.7460\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9721 - acc: 0.7671 - val_loss: 0.9876 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9712 - acc: 0.7688 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9705 - acc: 0.7691 - val_loss: 0.9740 - val_acc: 0.7520\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.7700 - val_loss: 0.9739 - val_acc: 0.7550\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7691 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9683 - acc: 0.7680 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9672 - acc: 0.7667 - val_loss: 0.9804 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9665 - acc: 0.7671 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9658 - acc: 0.7677 - val_loss: 0.9884 - val_acc: 0.7550\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.7697 - val_loss: 0.9716 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9652 - acc: 0.7655 - val_loss: 0.9746 - val_acc: 0.7520\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9644 - acc: 0.7688 - val_loss: 0.9681 - val_acc: 0.7560\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9641 - acc: 0.7685 - val_loss: 0.9718 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9729 - val_acc: 0.7540\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9627 - acc: 0.7673 - val_loss: 0.9664 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9621 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7699 - val_loss: 0.9635 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7688 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9603 - acc: 0.7695 - val_loss: 0.9697 - val_acc: 0.7480\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9605 - acc: 0.7669 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7693 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9586 - acc: 0.7687 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7683 - val_loss: 0.9633 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9576 - acc: 0.7681 - val_loss: 0.9751 - val_acc: 0.7540\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.7683 - val_loss: 0.9652 - val_acc: 0.7600\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9572 - acc: 0.7700 - val_loss: 0.9627 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.7677 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7630\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9555 - acc: 0.7679 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7692 - val_loss: 0.9640 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7700 - val_loss: 0.9772 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9538 - acc: 0.7713 - val_loss: 0.9612 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9576 - val_acc: 0.7620\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9510 - acc: 0.7699 - val_loss: 0.9588 - val_acc: 0.7510\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.7692 - val_loss: 0.9565 - val_acc: 0.7580\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9517 - acc: 0.7689 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9504 - acc: 0.7700 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7680 - val_loss: 0.9571 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9496 - acc: 0.7681 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7712 - val_loss: 0.9659 - val_acc: 0.7510\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9487 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9483 - acc: 0.7709 - val_loss: 0.9521 - val_acc: 0.7580\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9468 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7620\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9469 - acc: 0.7689 - val_loss: 0.9570 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9463 - acc: 0.7717 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9472 - acc: 0.7719 - val_loss: 0.9545 - val_acc: 0.7610\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9461 - acc: 0.7701 - val_loss: 0.9621 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9448 - acc: 0.7716 - val_loss: 0.9535 - val_acc: 0.7650\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9450 - acc: 0.7705 - val_loss: 0.9510 - val_acc: 0.7590\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7703 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9434 - acc: 0.7708 - val_loss: 0.9491 - val_acc: 0.7660\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7720 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9417 - acc: 0.7707 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9429 - acc: 0.7719 - val_loss: 0.9516 - val_acc: 0.7600\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9419 - acc: 0.7729 - val_loss: 0.9532 - val_acc: 0.7620\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9450 - acc: 0.770 - 0s 44us/step - loss: 0.9427 - acc: 0.7716 - val_loss: 0.9465 - val_acc: 0.7610\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9407 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7570\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7712 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9392 - acc: 0.7729 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.7712 - val_loss: 0.9449 - val_acc: 0.7540\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.7731 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9381 - acc: 0.7711 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.7708 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7630\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9374 - acc: 0.7713 - val_loss: 0.9447 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9356 - acc: 0.7717 - val_loss: 0.9426 - val_acc: 0.7650\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7747 - val_loss: 0.9450 - val_acc: 0.7640\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9352 - acc: 0.7708 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7724 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7739 - val_loss: 0.9410 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9336 - acc: 0.7701 - val_loss: 0.9415 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9341 - acc: 0.7712 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9335 - acc: 0.7724 - val_loss: 0.9444 - val_acc: 0.7510\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9326 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9327 - acc: 0.7732 - val_loss: 0.9391 - val_acc: 0.7640\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9313 - acc: 0.7736 - val_loss: 0.9389 - val_acc: 0.7680\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9310 - acc: 0.7736 - val_loss: 0.9373 - val_acc: 0.7640\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9310 - acc: 0.7708 - val_loss: 0.9374 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.7748 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9297 - acc: 0.7739 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9301 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9287 - acc: 0.7724 - val_loss: 0.9380 - val_acc: 0.7680\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9389 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9360 - val_acc: 0.7630\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7733 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9268 - acc: 0.7725 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.7736 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7729 - val_loss: 0.9420 - val_acc: 0.7660\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7745 - val_loss: 0.9486 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9260 - acc: 0.7741 - val_loss: 0.9360 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7737 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9237 - acc: 0.7745 - val_loss: 0.9386 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9235 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9235 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7725 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9232 - acc: 0.7736 - val_loss: 0.9312 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9228 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7630\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9222 - acc: 0.7721 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9211 - acc: 0.7735 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9210 - acc: 0.7735 - val_loss: 0.9288 - val_acc: 0.7670\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7720 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7763 - val_loss: 0.9447 - val_acc: 0.7620\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7756 - val_loss: 0.9429 - val_acc: 0.7510\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7733 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7737 - val_loss: 0.9272 - val_acc: 0.7650\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9191 - acc: 0.7760 - val_loss: 0.9390 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9193 - acc: 0.7736 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9188 - acc: 0.7751 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9179 - acc: 0.7748 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9184 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7670\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7690\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9168 - acc: 0.7740 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 0.9301 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7763 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9161 - acc: 0.7743 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7759 - val_loss: 0.9281 - val_acc: 0.7530\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9152 - acc: 0.7751 - val_loss: 0.9285 - val_acc: 0.7660\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7756 - val_loss: 0.9289 - val_acc: 0.7560\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.7756 - val_loss: 0.9243 - val_acc: 0.7690\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9138 - acc: 0.7768 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9137 - acc: 0.7743 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9131 - acc: 0.7760 - val_loss: 0.9238 - val_acc: 0.7650\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9133 - acc: 0.7751 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.7724 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9137 - acc: 0.7729 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7757 - val_loss: 0.9300 - val_acc: 0.7560\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9125 - acc: 0.7745 - val_loss: 0.9280 - val_acc: 0.7640\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7727 - val_loss: 0.9250 - val_acc: 0.7630\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9114 - acc: 0.7753 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.7728 - val_loss: 0.9228 - val_acc: 0.7650\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9119 - acc: 0.7747 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9101 - acc: 0.7753 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9103 - acc: 0.7777 - val_loss: 0.9350 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9286 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.7740 - val_loss: 0.9217 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7751 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9097 - acc: 0.7771 - val_loss: 0.9202 - val_acc: 0.7650\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9076 - acc: 0.7768 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9082 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7640\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9085 - acc: 0.7753 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9064 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7660\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9062 - acc: 0.7765 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9070 - acc: 0.7749 - val_loss: 0.9217 - val_acc: 0.7680\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7764 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7761 - val_loss: 0.9189 - val_acc: 0.7640\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7767 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7580\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.7783 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7772 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9046 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9037 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9035 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7773 - val_loss: 0.9202 - val_acc: 0.7660\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7767 - val_loss: 0.9192 - val_acc: 0.7660\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9171 - val_acc: 0.7670\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9024 - acc: 0.7784 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9019 - acc: 0.7775 - val_loss: 0.9148 - val_acc: 0.7670\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7773 - val_loss: 0.9198 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9014 - acc: 0.7776 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7767 - val_loss: 0.9141 - val_acc: 0.7670\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7781 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7745 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8994 - acc: 0.7773 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7777 - val_loss: 0.9187 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9140 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8999 - acc: 0.7775 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7792 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8992 - acc: 0.7785 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8987 - acc: 0.7788 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7780 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8975 - acc: 0.7781 - val_loss: 0.9128 - val_acc: 0.7690\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8969 - acc: 0.7769 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9196 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8989 - acc: 0.7796 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8973 - acc: 0.7791 - val_loss: 0.9126 - val_acc: 0.7600\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7781 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8961 - acc: 0.7791 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8964 - acc: 0.7771 - val_loss: 0.9090 - val_acc: 0.7660\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7788 - val_loss: 0.9136 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8959 - acc: 0.7793 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8955 - acc: 0.7792 - val_loss: 0.9107 - val_acc: 0.7670\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8950 - acc: 0.7809 - val_loss: 0.9091 - val_acc: 0.7700\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8940 - acc: 0.7764 - val_loss: 0.9135 - val_acc: 0.7680\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8941 - acc: 0.7785 - val_loss: 0.9099 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8928 - acc: 0.7781 - val_loss: 0.9082 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7785 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8926 - acc: 0.7807 - val_loss: 0.9074 - val_acc: 0.7710\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7772 - val_loss: 0.9051 - val_acc: 0.7680\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8925 - acc: 0.7781 - val_loss: 0.9156 - val_acc: 0.7650\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7793 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8929 - acc: 0.7780 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8921 - acc: 0.7788 - val_loss: 0.9118 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9090 - val_acc: 0.7700\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7797 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8922 - acc: 0.7783 - val_loss: 0.9152 - val_acc: 0.7520\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8911 - acc: 0.7776 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7783 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7773 - val_loss: 0.9061 - val_acc: 0.7660\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7680\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8894 - acc: 0.7792 - val_loss: 0.9070 - val_acc: 0.7620\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8905 - acc: 0.7783 - val_loss: 0.9043 - val_acc: 0.7690\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.7793 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7788 - val_loss: 0.9194 - val_acc: 0.7610\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8890 - acc: 0.7799 - val_loss: 0.9028 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7804 - val_loss: 0.9060 - val_acc: 0.7610\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7793 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8895 - acc: 0.7792 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7779 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8875 - acc: 0.7791 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8873 - acc: 0.7819 - val_loss: 0.8998 - val_acc: 0.7700\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8862 - acc: 0.7803 - val_loss: 0.9126 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8866 - acc: 0.7807 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7803 - val_loss: 0.9050 - val_acc: 0.7620\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9017 - val_acc: 0.7600\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8861 - acc: 0.7813 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9049 - val_acc: 0.7630\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7808 - val_loss: 0.9038 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8863 - acc: 0.7809 - val_loss: 0.9004 - val_acc: 0.7650\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9061 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7819 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7815 - val_loss: 0.9010 - val_acc: 0.7630\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.7600\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8851 - acc: 0.7827 - val_loss: 0.9006 - val_acc: 0.7710\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7805 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8848 - acc: 0.7805 - val_loss: 0.9001 - val_acc: 0.7680\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8849 - acc: 0.7813 - val_loss: 0.8981 - val_acc: 0.7710\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7795 - val_loss: 0.9019 - val_acc: 0.7670\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7600\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7813 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8840 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7812 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8823 - acc: 0.7809 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8825 - acc: 0.7797 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7816 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8814 - acc: 0.7823 - val_loss: 0.9037 - val_acc: 0.7570\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.8956 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8825 - acc: 0.7803 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.8978 - val_acc: 0.7700\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8805 - acc: 0.7829 - val_loss: 0.9006 - val_acc: 0.7690\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8809 - acc: 0.7815 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8807 - acc: 0.7827 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8810 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.7720\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8803 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.7825 - val_loss: 0.9025 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8799 - acc: 0.7827 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8796 - acc: 0.7824 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.8961 - val_acc: 0.7670\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8789 - acc: 0.781 - 0s 36us/step - loss: 0.8798 - acc: 0.7816 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8789 - acc: 0.7817 - val_loss: 0.9079 - val_acc: 0.7670\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8780 - acc: 0.7828 - val_loss: 0.8931 - val_acc: 0.7660\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8794 - acc: 0.7817 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7829 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7803 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8778 - acc: 0.7827 - val_loss: 0.9133 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8782 - acc: 0.7827 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9114 - val_acc: 0.7610\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8767 - acc: 0.7835 - val_loss: 0.9009 - val_acc: 0.7680\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8770 - acc: 0.7852 - val_loss: 0.8958 - val_acc: 0.7730\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.7804 - val_loss: 0.8937 - val_acc: 0.7740\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8775 - acc: 0.7815 - val_loss: 0.8947 - val_acc: 0.7720\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7839 - val_loss: 0.8932 - val_acc: 0.7710\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8765 - acc: 0.7823 - val_loss: 0.8949 - val_acc: 0.7690\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8767 - acc: 0.7823 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8755 - acc: 0.7831 - val_loss: 0.8939 - val_acc: 0.7660\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8768 - acc: 0.7817 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.8954 - val_acc: 0.7690\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8759 - acc: 0.7800 - val_loss: 0.9110 - val_acc: 0.7630\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.7825 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8749 - acc: 0.7833 - val_loss: 0.8952 - val_acc: 0.7670\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.8930 - val_acc: 0.7680\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8744 - acc: 0.7843 - val_loss: 0.8956 - val_acc: 0.7630\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8740 - acc: 0.7827 - val_loss: 0.8945 - val_acc: 0.7690\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8744 - acc: 0.7829 - val_loss: 0.8928 - val_acc: 0.7660\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8747 - acc: 0.7817 - val_loss: 0.8969 - val_acc: 0.7700\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7829 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8726 - acc: 0.7845 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8742 - acc: 0.7841 - val_loss: 0.8928 - val_acc: 0.7720\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8733 - acc: 0.7853 - val_loss: 0.8896 - val_acc: 0.7700\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8725 - acc: 0.7813 - val_loss: 0.8941 - val_acc: 0.7650\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7840 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.7815 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7836 - val_loss: 0.8885 - val_acc: 0.7710\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8718 - acc: 0.7837 - val_loss: 0.8904 - val_acc: 0.7700\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8715 - acc: 0.7837 - val_loss: 0.8936 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.8924 - val_acc: 0.7670\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7855 - val_loss: 0.8929 - val_acc: 0.7710\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7835 - val_loss: 0.8895 - val_acc: 0.7740\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7851 - val_loss: 0.9019 - val_acc: 0.7520\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7836 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7833 - val_loss: 0.8919 - val_acc: 0.7690\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7847 - val_loss: 0.8945 - val_acc: 0.7730\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8701 - acc: 0.7823 - val_loss: 0.8916 - val_acc: 0.7700\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7828 - val_loss: 0.8978 - val_acc: 0.7550\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8698 - acc: 0.7824 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8692 - acc: 0.7851 - val_loss: 0.8976 - val_acc: 0.7690\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7844 - val_loss: 0.8921 - val_acc: 0.7620\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8695 - acc: 0.7840 - val_loss: 0.8903 - val_acc: 0.7720\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7839 - val_loss: 0.8968 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.8901 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8685 - acc: 0.7851 - val_loss: 0.8876 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.7837 - val_loss: 0.9169 - val_acc: 0.7610\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.7831 - val_loss: 0.8896 - val_acc: 0.7770\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8682 - acc: 0.7828 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7803 - val_loss: 0.8892 - val_acc: 0.7680\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8687 - acc: 0.7824 - val_loss: 0.8875 - val_acc: 0.7770\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8687 - acc: 0.7839 - val_loss: 0.8971 - val_acc: 0.7640\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7848 - val_loss: 0.8876 - val_acc: 0.7740\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7847 - val_loss: 0.9123 - val_acc: 0.7540\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7851 - val_loss: 0.8878 - val_acc: 0.7690\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8683 - acc: 0.7845 - val_loss: 0.8944 - val_acc: 0.7520\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8670 - acc: 0.7845 - val_loss: 0.8923 - val_acc: 0.7690\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.7837 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.8988 - val_acc: 0.7700\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7851 - val_loss: 0.8952 - val_acc: 0.7700\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7843 - val_loss: 0.8854 - val_acc: 0.7690\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7835 - val_loss: 0.8869 - val_acc: 0.7720\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7824 - val_loss: 0.9022 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.7831 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8654 - acc: 0.7859 - val_loss: 0.8909 - val_acc: 0.7730\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7841 - val_loss: 0.8867 - val_acc: 0.7720\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7823 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.8929 - val_acc: 0.7740\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8648 - acc: 0.7843 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8660 - acc: 0.7836 - val_loss: 0.8881 - val_acc: 0.7660\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7864 - val_loss: 0.8970 - val_acc: 0.7670\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8653 - acc: 0.7841 - val_loss: 0.8851 - val_acc: 0.7760\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7836 - val_loss: 0.8896 - val_acc: 0.7760\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7841 - val_loss: 0.9073 - val_acc: 0.7550\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8668 - acc: 0.7856 - val_loss: 0.8923 - val_acc: 0.7630\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8658 - acc: 0.7857 - val_loss: 0.8906 - val_acc: 0.7760\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7855 - val_loss: 0.8862 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8647 - acc: 0.7861 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8655 - acc: 0.7853 - val_loss: 0.8893 - val_acc: 0.7700\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.7832 - val_loss: 0.8870 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7843 - val_loss: 0.8907 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7856 - val_loss: 0.8835 - val_acc: 0.7760\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7845 - val_loss: 0.8857 - val_acc: 0.7690\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8629 - acc: 0.7832 - val_loss: 0.8880 - val_acc: 0.7660\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8625 - acc: 0.7867 - val_loss: 0.8946 - val_acc: 0.7670\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8632 - acc: 0.7867 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7844 - val_loss: 0.8890 - val_acc: 0.7710\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8626 - acc: 0.7867 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8642 - acc: 0.7847 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.8846 - val_acc: 0.7710\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.8870 - val_acc: 0.7670\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8630 - acc: 0.7855 - val_loss: 0.8827 - val_acc: 0.7710\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8619 - acc: 0.7847 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8638 - acc: 0.7860 - val_loss: 0.8872 - val_acc: 0.7560\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8615 - acc: 0.7848 - val_loss: 0.8840 - val_acc: 0.7770\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8606 - acc: 0.7865 - val_loss: 0.8842 - val_acc: 0.7670\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8600 - acc: 0.7875 - val_loss: 0.8842 - val_acc: 0.7740\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7840 - val_loss: 0.8854 - val_acc: 0.7720\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8617 - acc: 0.7847 - val_loss: 0.8840 - val_acc: 0.7750\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7855 - val_loss: 0.8852 - val_acc: 0.7750\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8610 - acc: 0.7844 - val_loss: 0.8845 - val_acc: 0.7740\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8604 - acc: 0.7869 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8603 - acc: 0.7865 - val_loss: 0.8844 - val_acc: 0.7720\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8603 - acc: 0.7857 - val_loss: 0.8933 - val_acc: 0.7670\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8608 - acc: 0.7860 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.8915 - val_acc: 0.7640\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8605 - acc: 0.7867 - val_loss: 0.8816 - val_acc: 0.7760\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7833 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8585 - acc: 0.7853 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7852 - val_loss: 0.8810 - val_acc: 0.7760\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.7859 - val_loss: 0.8847 - val_acc: 0.7660\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8604 - acc: 0.7852 - val_loss: 0.8929 - val_acc: 0.7670\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8613 - acc: 0.7852 - val_loss: 0.8826 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.7857 - val_loss: 0.8836 - val_acc: 0.7750\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7865 - val_loss: 0.8780 - val_acc: 0.7750\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8584 - acc: 0.7837 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.7856 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8833 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7867 - val_loss: 0.8867 - val_acc: 0.7670\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7884 - val_loss: 0.8854 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8580 - acc: 0.7863 - val_loss: 0.8930 - val_acc: 0.7600\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7864 - val_loss: 0.8848 - val_acc: 0.7670\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.8811 - val_acc: 0.7800\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8580 - acc: 0.7875 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7884 - val_loss: 0.8864 - val_acc: 0.7630\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8568 - acc: 0.7860 - val_loss: 0.8818 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8582 - acc: 0.7865 - val_loss: 0.8820 - val_acc: 0.7780\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.7875 - val_loss: 0.8899 - val_acc: 0.7640\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7865 - val_loss: 0.8828 - val_acc: 0.7630\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8559 - acc: 0.7872 - val_loss: 0.8801 - val_acc: 0.7730\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8576 - acc: 0.7848 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8566 - acc: 0.7859 - val_loss: 0.8860 - val_acc: 0.7610\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.7876 - val_loss: 0.8777 - val_acc: 0.7790\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7885 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.7881 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7861 - val_loss: 0.8865 - val_acc: 0.7650\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8570 - acc: 0.7872 - val_loss: 0.8798 - val_acc: 0.7790\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8555 - acc: 0.7875 - val_loss: 0.8794 - val_acc: 0.7690\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8556 - acc: 0.7888 - val_loss: 0.8802 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8554 - acc: 0.7879 - val_loss: 0.8878 - val_acc: 0.7720\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.8827 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.8850 - val_acc: 0.7740\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8555 - acc: 0.7856 - val_loss: 0.8881 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8531 - acc: 0.7888 - val_loss: 0.8868 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.8804 - val_acc: 0.7740\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8552 - acc: 0.7867 - val_loss: 0.8799 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8550 - acc: 0.7875 - val_loss: 0.8773 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.7867 - val_loss: 0.8778 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8556 - acc: 0.7880 - val_loss: 0.8768 - val_acc: 0.7740\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8545 - acc: 0.7876 - val_loss: 0.8845 - val_acc: 0.7570\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8534 - acc: 0.7879 - val_loss: 0.8850 - val_acc: 0.7700\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8546 - acc: 0.7881 - val_loss: 0.8927 - val_acc: 0.7640\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8547 - acc: 0.7888 - val_loss: 0.8765 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8542 - acc: 0.7877 - val_loss: 0.8846 - val_acc: 0.7620\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.8797 - val_acc: 0.7690\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8535 - acc: 0.7871 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.9048 - val_acc: 0.7520\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7857 - val_loss: 0.8794 - val_acc: 0.7720\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8543 - acc: 0.7877 - val_loss: 0.8813 - val_acc: 0.7730\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.7877 - val_loss: 0.8790 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7790\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7885 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7875 - val_loss: 0.8800 - val_acc: 0.7770\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7863 - val_loss: 0.8798 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7883 - val_loss: 0.8860 - val_acc: 0.7620\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.7867 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7897 - val_loss: 0.8847 - val_acc: 0.7670\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8513 - acc: 0.7877 - val_loss: 0.8908 - val_acc: 0.7630\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8515 - acc: 0.7877 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.7869 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8524 - acc: 0.7879 - val_loss: 0.8761 - val_acc: 0.7690\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8807 - val_acc: 0.7690\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.8847 - val_acc: 0.7680\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.8848 - val_acc: 0.7630\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8516 - acc: 0.7891 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8502 - acc: 0.7880 - val_loss: 0.8824 - val_acc: 0.7660\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7897 - val_loss: 0.8898 - val_acc: 0.7520\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.7885 - val_loss: 0.8823 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.7871 - val_loss: 0.8852 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8866 - val_acc: 0.7630\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7880 - val_loss: 0.9056 - val_acc: 0.7660\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.8780 - val_acc: 0.7710\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8492 - acc: 0.7908 - val_loss: 0.9256 - val_acc: 0.7600\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8505 - acc: 0.7871 - val_loss: 0.8952 - val_acc: 0.7630\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8510 - acc: 0.7888 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.7888 - val_loss: 0.8754 - val_acc: 0.7660\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.7873 - val_loss: 0.8806 - val_acc: 0.7650\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.8784 - val_acc: 0.7610\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.7896 - val_loss: 0.8749 - val_acc: 0.7730\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7883 - val_loss: 0.8865 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8503 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7730\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7860 - val_loss: 0.8727 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7810\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8500 - acc: 0.7881 - val_loss: 0.8779 - val_acc: 0.7670\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7875 - val_loss: 0.8777 - val_acc: 0.7730\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.8754 - val_acc: 0.7670\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8485 - acc: 0.7908 - val_loss: 0.8877 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7869 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8477 - acc: 0.7885 - val_loss: 0.8737 - val_acc: 0.7710\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.8722 - val_acc: 0.7660\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7900 - val_loss: 0.8985 - val_acc: 0.7710\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.7865 - val_loss: 0.8861 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8490 - acc: 0.7897 - val_loss: 0.8863 - val_acc: 0.7690\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7872 - val_loss: 0.8863 - val_acc: 0.7790\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8472 - acc: 0.7884 - val_loss: 0.8747 - val_acc: 0.7760\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7921 - val_loss: 0.8806 - val_acc: 0.7730\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8463 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.7888 - val_loss: 0.8986 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.7880 - val_loss: 0.8724 - val_acc: 0.7720\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.7911 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.7901 - val_loss: 0.8803 - val_acc: 0.7680\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7919 - val_loss: 0.8756 - val_acc: 0.7760\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8457 - acc: 0.7896 - val_loss: 0.8704 - val_acc: 0.7780\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7897 - val_loss: 0.8741 - val_acc: 0.7680\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8451 - acc: 0.7889 - val_loss: 0.8812 - val_acc: 0.7730\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.7888 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8459 - acc: 0.7901 - val_loss: 0.8758 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.8855 - val_acc: 0.7680\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7895 - val_loss: 0.8714 - val_acc: 0.7760\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.7931 - val_loss: 0.8870 - val_acc: 0.7630\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7917 - val_loss: 0.8808 - val_acc: 0.7760\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8469 - acc: 0.7899 - val_loss: 0.8759 - val_acc: 0.7670\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8457 - acc: 0.7880 - val_loss: 0.8780 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7921 - val_loss: 0.8760 - val_acc: 0.7710\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.8776 - val_acc: 0.7700\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8449 - acc: 0.7893 - val_loss: 0.8746 - val_acc: 0.7680\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8450 - acc: 0.7915 - val_loss: 0.8837 - val_acc: 0.7720\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.7911 - val_loss: 0.8805 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7899 - val_loss: 0.8708 - val_acc: 0.7720\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8440 - acc: 0.7892 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8433 - acc: 0.7921 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8447 - acc: 0.7893 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.7905 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.7887 - val_loss: 0.8811 - val_acc: 0.7710\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8439 - acc: 0.7887 - val_loss: 0.8814 - val_acc: 0.7660\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8444 - acc: 0.7895 - val_loss: 0.9006 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8427 - acc: 0.7885 - val_loss: 0.8728 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8431 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7899 - val_loss: 0.8770 - val_acc: 0.7700\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8447 - acc: 0.7880 - val_loss: 0.8745 - val_acc: 0.7650\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8428 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7760\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8425 - acc: 0.7915 - val_loss: 0.8753 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7893 - val_loss: 0.8756 - val_acc: 0.7630\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8419 - acc: 0.7913 - val_loss: 0.8763 - val_acc: 0.7740\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8435 - acc: 0.7900 - val_loss: 0.8850 - val_acc: 0.7780\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.8700 - val_acc: 0.7760\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8421 - acc: 0.7883 - val_loss: 0.8707 - val_acc: 0.7790\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.8699 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.7919 - val_loss: 0.8727 - val_acc: 0.7700\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8420 - acc: 0.7923 - val_loss: 0.8736 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8838 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.7917 - val_loss: 0.8744 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7892 - val_loss: 0.8892 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8431 - acc: 0.7903 - val_loss: 0.8729 - val_acc: 0.7710\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.7931 - val_loss: 0.8717 - val_acc: 0.7700\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8413 - acc: 0.7903 - val_loss: 0.8742 - val_acc: 0.7740\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7908 - val_loss: 0.8823 - val_acc: 0.7750\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8423 - acc: 0.7908 - val_loss: 0.8700 - val_acc: 0.7700\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7907 - val_loss: 0.8698 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8405 - acc: 0.7913 - val_loss: 0.8691 - val_acc: 0.7740\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8429 - acc: 0.7896 - val_loss: 0.8698 - val_acc: 0.7740\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7912 - val_loss: 0.8722 - val_acc: 0.7700\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.7901 - val_loss: 0.9618 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7873 - val_loss: 0.8821 - val_acc: 0.7790\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.8696 - val_acc: 0.7820\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8402 - acc: 0.7912 - val_loss: 0.8697 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8730 - val_acc: 0.7640\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.8805 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8410 - acc: 0.7911 - val_loss: 0.8692 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8393 - acc: 0.7912 - val_loss: 0.8701 - val_acc: 0.7720\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.7904 - val_loss: 0.8670 - val_acc: 0.7790\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7921 - val_loss: 0.8851 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8400 - acc: 0.7905 - val_loss: 0.8999 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7907 - val_loss: 0.8827 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8418 - acc: 0.7880 - val_loss: 0.8833 - val_acc: 0.7710\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8380 - acc: 0.7939 - val_loss: 0.8869 - val_acc: 0.7600\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.7927 - val_loss: 0.8714 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.8788 - val_acc: 0.7680\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.7909 - val_loss: 0.8762 - val_acc: 0.7700\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7908 - val_loss: 0.8687 - val_acc: 0.7720\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.8948 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7925 - val_loss: 0.8860 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8389 - acc: 0.7901 - val_loss: 0.8712 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7921 - val_loss: 0.8751 - val_acc: 0.7640\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8391 - acc: 0.7933 - val_loss: 0.8676 - val_acc: 0.7740\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7920 - val_loss: 0.8739 - val_acc: 0.7710\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8379 - acc: 0.7924 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8370 - acc: 0.7909 - val_loss: 0.8694 - val_acc: 0.7780\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7924 - val_loss: 0.8706 - val_acc: 0.7780\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7919 - val_loss: 0.8774 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8364 - acc: 0.7953 - val_loss: 0.8750 - val_acc: 0.7700\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8387 - acc: 0.7908 - val_loss: 0.8785 - val_acc: 0.7660\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8378 - acc: 0.7920 - val_loss: 0.8692 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8357 - acc: 0.7912 - val_loss: 0.8671 - val_acc: 0.7770\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8378 - acc: 0.7921 - val_loss: 0.8758 - val_acc: 0.7720\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8380 - acc: 0.7924 - val_loss: 0.8648 - val_acc: 0.7760\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8372 - acc: 0.7929 - val_loss: 0.8806 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.8671 - val_acc: 0.7760\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8381 - acc: 0.7924 - val_loss: 0.8780 - val_acc: 0.7590\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8364 - acc: 0.7929 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8360 - acc: 0.7935 - val_loss: 0.8719 - val_acc: 0.7640\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8375 - acc: 0.7937 - val_loss: 0.8741 - val_acc: 0.7720\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8369 - acc: 0.7924 - val_loss: 0.8723 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.7937 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8361 - acc: 0.7883 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.7940 - val_loss: 0.8714 - val_acc: 0.7690\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8356 - acc: 0.7939 - val_loss: 0.8732 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8354 - acc: 0.7935 - val_loss: 0.8716 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8361 - acc: 0.7916 - val_loss: 0.8665 - val_acc: 0.7740\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.8713 - val_acc: 0.7790\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8351 - acc: 0.7912 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8370 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7931 - val_loss: 0.8860 - val_acc: 0.7580\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.8719 - val_acc: 0.7750\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7935 - val_loss: 0.8811 - val_acc: 0.7600\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8350 - acc: 0.7932 - val_loss: 0.8670 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8361 - acc: 0.7917 - val_loss: 0.8775 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.7927 - val_loss: 0.8785 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7931 - val_loss: 0.8660 - val_acc: 0.7820\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8348 - acc: 0.7925 - val_loss: 0.8749 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8355 - acc: 0.7924 - val_loss: 0.8714 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8336 - acc: 0.7924 - val_loss: 0.8703 - val_acc: 0.7630\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8331 - acc: 0.7953 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.8785 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8721 - val_acc: 0.7800\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8345 - acc: 0.7949 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.8663 - val_acc: 0.7790\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8332 - acc: 0.7947 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8328 - acc: 0.7931 - val_loss: 0.8873 - val_acc: 0.7640\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8343 - acc: 0.7940 - val_loss: 0.8788 - val_acc: 0.7690\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8333 - acc: 0.7933 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.7937 - val_loss: 0.8792 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7905 - val_loss: 0.8708 - val_acc: 0.7700\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8318 - acc: 0.7947 - val_loss: 0.8702 - val_acc: 0.7780\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.7961 - val_loss: 0.8727 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7940 - val_loss: 0.8657 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7917 - val_loss: 0.8748 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.7956 - val_loss: 0.8673 - val_acc: 0.7710\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8332 - acc: 0.7928 - val_loss: 0.8731 - val_acc: 0.7700\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.8649 - val_acc: 0.7750\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8340 - acc: 0.7935 - val_loss: 0.8692 - val_acc: 0.7730\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8743 - val_acc: 0.7750\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7935 - val_loss: 0.8815 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8325 - acc: 0.7924 - val_loss: 0.8768 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8309 - acc: 0.7928 - val_loss: 0.8755 - val_acc: 0.7800\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.8750 - val_acc: 0.7760\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7943 - val_loss: 0.8758 - val_acc: 0.7630\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.8810 - val_acc: 0.7650\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.8650 - val_acc: 0.7790\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8313 - acc: 0.7931 - val_loss: 0.8739 - val_acc: 0.7790\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8301 - acc: 0.7935 - val_loss: 0.8809 - val_acc: 0.7660\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7952 - val_loss: 0.8729 - val_acc: 0.7660\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7915 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8314 - acc: 0.7937 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.7944 - val_loss: 0.8653 - val_acc: 0.7800\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8303 - acc: 0.7969 - val_loss: 0.8620 - val_acc: 0.7780\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8307 - acc: 0.7940 - val_loss: 0.9117 - val_acc: 0.7530\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8722 - val_acc: 0.7640\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8318 - acc: 0.7956 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7940 - val_loss: 0.8882 - val_acc: 0.7560\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7953 - val_loss: 0.8652 - val_acc: 0.7710\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8320 - acc: 0.7960 - val_loss: 0.8838 - val_acc: 0.7720\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7943 - val_loss: 0.8648 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8288 - acc: 0.7957 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8293 - acc: 0.7955 - val_loss: 0.8815 - val_acc: 0.7700\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.7967 - val_loss: 0.8624 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.7955 - val_loss: 0.8666 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.7936 - val_loss: 0.8820 - val_acc: 0.7740\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.7951 - val_loss: 0.8776 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8311 - acc: 0.7921 - val_loss: 0.9091 - val_acc: 0.7580\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8301 - acc: 0.7952 - val_loss: 0.8609 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7951 - val_loss: 0.8673 - val_acc: 0.7700\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8315 - acc: 0.7940 - val_loss: 0.8695 - val_acc: 0.7740\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7948 - val_loss: 0.8662 - val_acc: 0.7700\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7959 - val_loss: 0.8629 - val_acc: 0.7800\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8621 - val_acc: 0.7790\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8309 - acc: 0.7948 - val_loss: 0.8687 - val_acc: 0.7730\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.7956 - val_loss: 0.8656 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8275 - acc: 0.7956 - val_loss: 0.8643 - val_acc: 0.7700\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7937 - val_loss: 0.8645 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8661 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7937 - val_loss: 0.8648 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8291 - acc: 0.7952 - val_loss: 0.8656 - val_acc: 0.7810\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7952 - val_loss: 0.8651 - val_acc: 0.7770\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8281 - acc: 0.7944 - val_loss: 0.8798 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.7987 - val_loss: 0.8627 - val_acc: 0.7750\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8275 - acc: 0.7961 - val_loss: 0.8654 - val_acc: 0.7710\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7956 - val_loss: 0.8720 - val_acc: 0.7670\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7943 - val_loss: 0.9056 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7955 - val_loss: 0.8746 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7947 - val_loss: 0.8683 - val_acc: 0.7670\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8262 - acc: 0.7971 - val_loss: 0.8672 - val_acc: 0.7750\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8269 - acc: 0.7943 - val_loss: 0.8624 - val_acc: 0.7740\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.7949 - val_loss: 0.8687 - val_acc: 0.7670\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7971 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.7960 - val_loss: 0.8981 - val_acc: 0.7720\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7923 - val_loss: 0.8765 - val_acc: 0.7760\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.7931 - val_loss: 0.8668 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7952 - val_loss: 0.8701 - val_acc: 0.7690\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8278 - acc: 0.7963 - val_loss: 0.8693 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7953 - val_loss: 0.8649 - val_acc: 0.7690\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7981 - val_loss: 0.8839 - val_acc: 0.7730\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.7940 - val_loss: 0.8716 - val_acc: 0.7770\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8268 - acc: 0.7971 - val_loss: 0.8685 - val_acc: 0.7830\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.7952 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7960 - val_loss: 0.8651 - val_acc: 0.7650\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8261 - acc: 0.7964 - val_loss: 0.8932 - val_acc: 0.7690\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8298 - acc: 0.7944 - val_loss: 0.8595 - val_acc: 0.7830\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8243 - acc: 0.7968 - val_loss: 0.8693 - val_acc: 0.7730\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8247 - acc: 0.7968 - val_loss: 0.8596 - val_acc: 0.7790\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.7988 - val_loss: 0.8680 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8262 - acc: 0.7965 - val_loss: 0.8655 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FfW5+PHPk4UkJEAgAaIkEgSUJYQtoggqLmVR3LVC9UpV9Fcr1q232taKeG+rt14Vt9vWXStqFRcQEVHEupQtKJF9MYCEQAghBMhCtuf3xwynh3CSnIScnJzkeb9e55UzM98z88yZyTzz/X7nzIiqYowxxgCEBTsAY4wxLYclBWOMMR6WFIwxxnhYUjDGGONhScEYY4yHJQVjjDEelhRaCBEJF5FDInJSU5Zt6UTkdRF50H0/RkTW+lO2EctpNd+ZaX7Hs++FGksKjeQeYI68qkWk1Gv42obOT1WrVDVOVX9syrKNISKnici3InJQRDaIyAWBWE5NqvqFqg5sinmJyNci8nOveQf0O2sLan6nXuP7i8hcEckXkX0i8rGI9A1CiKYJWFJoJPcAE6eqccCPwMVe42bVLC8iEc0fZaP9HzAX6AhcCOwMbjimNiISJiLB/j/uBHwAnAp0B1YB7zdnAC31/6uFbJ8GCalgQ4mI/LeI/ENE3hSRg8B1IjJSRJaKyH4R2SUiT4lIpFs+QkRURFLd4dfd6R+7Z+xLRKRXQ8u60yeIyCYRKRKRp0XkG19nfF4qge3qyFbV9fWs62YRGe813M49Y0x3/ylmi8hud72/EJH+tcznAhHZ5jU8XERWuev0JhDlNS1BROa7Z6eFIvKhiPRwp/0PMBL4q1tzm+njO4t3v7d8EdkmIr8VEXGnTRWRf4rIE27M2SIyto71v98tc1BE1orIJTWm/z+3xnVQRNaIyGB3fE8R+cCNYa+IPOmO/28RecXr831ERL2GvxaR/xKRJUAxcJIb83p3GT+IyNQaMVzhfpcHRGSLiIwVkckisqxGuXtFZHZt6+qLqi5V1ZdUdZ+qVgBPAANFpJOP72q0iOz0PlCKyNUi8q37/gxxaqkHRCRPRB71tcwj+4qI/E5EdgPPu+MvEZEsd7t9LSJpXp/J8Nqf3hKRd+TfTZdTReQLr7JH7S81ll3rvudOP2b7NOT7DDZLCoF1OfAGzpnUP3AOtncAicAoYDzw/+r4/M+APwBdcGoj/9XQsiLSDXgb+E93uVuBEfXEvRx47MjByw9vApO9hicAuar6vTs8D+gLJAFrgL/XN0MRiQLmAC/hrNMc4DKvImE4B4KTgJ5ABfAkgKreCywBfuHW3O70sYj/A9oDJwPnATcB13tNPxNYDSTgHORerCPcTTjbsxPwR+ANEenursdk4H7gWpya1xXAPnHObD8CtgCpQArOdvLXfwA3uvPMAfKAi9zhm4GnRSTdjeFMnO/xHiAeOBfYjnt2L0c39VyHH9unHmcDOapa5GPaNzjb6hyvcT/D+T8BeBp4VFU7An2AuhJUMhCHsw/8UkROw9knpuJst5eAOe5JShTO+r6Asz+9y9H7U0PUuu95qbl9Qoeq2us4X8A24IIa4/4b+Lyez/0aeMd9HwEokOoOvw781avsJcCaRpS9EfjKa5oAu4Cf1xLTdUAmTrNRDpDujp8ALKvlM/2AIiDaHf4H8Ltayia6scd6xf6g+/4CYJv7/jxgByBen11+pKyP+WYA+V7DX3uvo/d3BkTiJOhTvKbfBnzmvp8KbPCa1tH9bKKf+8Ma4CL3/SLgNh9lzgJ2A+E+pv038IrXcB/nX/WodXugnhjmHVkuTkJ7tJZyzwMz3PdDgL1AZC1lj/pOaylzEpALXF1HmUeA59z38UAJkOwO/wt4AEioZzkXAGVAuxrrMr1GuR9wEvZ5wI81pi312vemAl/42l9q7qd+7nt1bp+W/LKaQmDt8B4QkX4i8pHblHIAeAjnIFmb3V7vS3DOihpa9kTvONTZa+s6c7kDeEpV5+McKBe6Z5xnAp/5+oCqbsD557tIROKAibhnfuJc9fNnt3nlAM6ZMdS93kfiznHjPWL7kTciEisiL4jIj+58P/djnkd0A8K95+e+7+E1XPP7hFq+fxH5uVeTxX6cJHkklhSc76amFJwEWOVnzDXV3LcmisgycZrt9gNj/YgB4FWcWgw4JwT/UKcJqMHcWulC4ElVfaeOom8AV4rTdHolzsnGkX3yBmAAsFFElovIhXXMJ09Vy72GewL3HtkO7vdwAs52PZFj9/sdNIKf+16j5t0SWFIIrJq3oP0bzllkH3Wqxw/gnLkH0i6cajYAIiIcffCrKQLnLBpVnQPci5MMrgNm1vG5I01IlwOrVHWbO/56nFrHeTjNK32OhNKQuF3ebbO/AXoBI9zv8rwaZeu6/e8eoArnIOI97wZ3qIvIycBfgFtxzm7jgQ38e/12AL19fHQH0FNEwn1MK8Zp2joiyUcZ7z6GGJxmloeB7m4MC/2IAVX92p3HKJzt16imIxFJwNlPZqvq/9RVVp1mxV3AOI5uOkJVN6rqJJzE/RjwrohE1zarGsM7cGo98V6v9qr6Nr73pxSv9/5850fUt+/5ii1kWFJoXh1wmlmKxelsras/oanMA4aJyMVuO/YdQNc6yr8DPCgig9zOwA1AORAD1PbPCU5SmADcgtc/Oc46HwYKcP7p/uhn3F8DYSIyze30uxoYVmO+JUChe0B6oMbn83D6C47hngnPBv4kInHidMrfhdNE0FBxOAeAfJycOxWnpnDEC8BvRGSoOPqKSApOn0eBG0N7EYlxD8zgXL1zjoikiEg8cF89MUQB7dwYqkRkInC+1/QXgakicq44Hf/JInKq1/S/4yS2YlVdWs+yIkUk2usV6XYoL8RpLr2/ns8f8SbOdz4Sr34DEfkPEUlU1Wqc/xUFqv2c53PAbeJcUi3utr1YRGJx9qdwEbnV3Z+uBIZ7fTYLSHf3+xhgeh3LqW/fC2mWFJrXPcAU4CBOreEfgV6gquYB1wCP4xyEegPf4Ryoffkf4DWcS1L34dQOpuL8E38kIh1rWU4OTl/EGRzdYfoyThtzLrAWp83Yn7gP49Q6bgYKcTpoP/Aq8jhOzaPAnefHNWYxE5jsNiM87mMRv8RJdluBf+I0o7zmT2w14vweeAqnv2MXTkJY5jX9TZzv9B/AAeA9oLOqVuI0s/XHOcP9EbjK/dgCnEs6V7vznVtPDPtxDrDv42yzq3BOBo5M/xfO9/gUzoF2MUefJb8GpOFfLeE5oNTr9by7vGE4icf79zsn1jGfN3DOsD9V1UKv8RcC68W5Yu9/gWtqNBHVSlWX4dTY/oKzz2zCqeF670+/cKf9FJiP+3+gquuAPwFfABuBL+tYVH37XkiTo5tsTWvnNlfkAlep6lfBjscEn3smvQdIU9WtwY6nuYjISmCmqh7v1VatitUU2gARGS8indzL8v6A02ewPMhhmZbjNuCb1p4QxLmNSne3+egmnFrdwmDH1dK0yF8BmiY3GpiF0+68FrjMrU6bNk5EcnCus7802LE0g/44zXixOFdjXek2rxov1nxkjDHGw5qPjDHGeIRc81FiYqKmpqYGOwxjjAkpK1eu3KuqdV2ODoRgUkhNTSUzMzPYYRhjTEgRke31l7LmI2OMMV4sKRhjjPGwpGCMMcbDkoIxxhgPSwrGGGM8LCkYY4zxsKRgjDHGI+R+p2CMMa1BSUUJBSUFHCw/SFy7ODpGdSRMwiitKOVg+UG279/O1v1b2Vuyl4qqCiqqK7j4lIs5rcdpAY3LkoIxxhyHaq2mpKKEiqoKKqsrqaiuoKKqgpwDOXz141cszVlKr/heTDxlIid1Oom/f/93Xln1CjsONPyJnSd2ODHgSSHkboiXkZGh9otmY0wg7D60mx1FOxh2wjDCw5wnpRaUFLB422JyD+ay+9Duo155xXnkHcqjqo5Hbffp0ocdRTs4XOXcmFgQxvcZz9k9zyYhJoEOUR0oqSihqKyIKq0iNjKW2HaxpHRMoVfnXnSP7U5keCThEo7zNN3GEZGVqppRXzmrKRhjQoqvA7e3quoqqrWayPDIo8arKst2LmPuxrnsL9tPRVUFihIZ5pRbunMpq3avAqB7bHcu73c5OQdzWLBlAZXVlQBEhEXQPbY7SXFJnNDhBIYmDSUpLon46HgiwyOJCIsgMiySyPBIEmISODPlTLrGdqW4vJjPt35OdmE2V/S/gpROKbRUVlMwxjQrVWVjwUYKSgqICIsgJjKGvl36EhMZA8D2/dv56sev2Fq4lZwDORwsP0jHqI5ER0Tzrx3/YkXuCsA5cF/Z/0r6JTqPxN5ftp9vdnzDNzu+4VD5IWIiYugU3Ynusd3pHtedzQWb2bp/K5FhkXSK7kRkWCQiQmV1JZXVlQzqNohxvceR0imFORvn8NGmj0hon8DktMlc2f9K+nTpQ+eYzoRJaF6f429NwZKCMeYYqso3O74hMiySET1GICKeg3l5VTmDug3yNGUUlxezr3QfJ3Q4gYiwCDbs3cAHGz5gSc4SCkoK2Fe6j5jIGJLikgiXcJbkLGFvyd6jlhcmYfRL7EdZZRnZhdme8d1iu9ExqiMHDh/gUPkhBncfzMRTJtKzU08+2PgBH236iNLKUk/5tG5pnH3S2STFJXHg8AH2l+1nT8kedh3cRUL7BCYNnMRl/S6jU3Sner+DiqoKwsPCQzYJ1GRJwZg27uDhgyzNWUpxRTEVVRW0C29H97judIvthqo67diHi8g7lMee4j3EtYsjuWMy+0r38cg3j5CZ6/yf9YrvxXm9zuOrH79iU8EmwDlLH33SaLILs/k+73uqtIowCaNzdGcKSgsA6J/Yn6S4JLrEdKG0spTdh3ZTWlHKiB4jOLvn2aR0TKGiuoKDhw+yNn8tq3avIjwsnPNSz2NM6hj6JvQlOiK6znUsqyyjuLwYgKiIKOLaxQXwGw1t1qdgTIhSVXIO5NApuhMdozp6xh+uPMzCHxbyzrp3yMrLYkjSEEYmj6SssowVuSvYVLCJztGdSYpL4seiH/lmxzeetvCGOrnzyTw38TmiIqJ4Y/UbvLnmTUaljOLO0++kfWR7PvnhE5bkLKF35978dvRv6dGxB7kHc9l1cBdDkoZwab9LSe6Y3FRfSa2iI6LrTRymYQJaUxCR8cCTQDjwgqo+UmP6E8C57mB7oJuqxtc1T6spmJasrLKM/WX7KakoITIs8pgOxcrqSuZvns/Lq15m18FdJMUl0bV9V0+naM6BnKOaV7rEdKFLTBf2le6jsLQQRekc3ZmMEzPIystiT/EeAJI7JtM/sT8HDh9g96HddInpwrje4zj/5PNJbJ9IZFgkZZVl5BU7tYJwCScmMoYO7TqQFJdEt9huHCo/xI4DO6ioquD8k88nIszOGVuToNcURCQceBb4CZADrBCRuaq67kgZVb3Lq/ztwNBAxWNMY+0r3cf8zfMJl3BPh+eR9vSo8CjaR7Zn6/6tvLH6DeZtmue59BCcM+5xvccRHRHNpoJNZOZmklecR1JcEmnd0sguzGbZzmVUVTuXNCa0T2DiKRPJOCGD4opithZupbCskISYBBLaJzAyeSTnn3w+7cLboapsL9pOdEQ0SXFJTbKufRP6Nsl82jKZIeh09TnO17T65tOQzzSFgNUURGQk8KCqjnOHfwugqg/XUv5fwHRV/bSu+VpNwRwvVeVg+UF2H9rNnuI9FJQUUFBaQGRYJAntE+gU1YnDVYc5VH6IDzd+yOurX6essqze+XaL7cY1A69hQNcBxETEUHS4iE+zP2Xx1sVUazV9E/oyoOsAJg2cxIV9Lzzmksm2pqkOdnUdhGsb19ADrq/P1rcO9c3bVwyBTABB72gWkauA8ao61R3+D+B0VZ3mo2xPYCmQrHrsr0BE5BbgFoCTTjpp+Pbtfj1VzrRx1VrNqt2rWJe/jk0Fmzyvzfs2c6j8kF/ziImI4br067hl+C3EtYvjwOEDngShqhyuOkxpRSkdozpyVs+zfDa5VFZXEiZhIXkVS3OfpR6P4421uQ/SjXE88bSEpHA1MK5GUhihqrf7KHsvTkI4ZlpNVlMw+cX5fJb9Gend0+nftf8xB9uVuSt54dsXmLNxDrsO7QKcSx5T41M5NeFU+nbpS0qnFE9bemL7RLrEdKGiqoKC0gIOHD5AVHgUMZExnJJwCvHRdXZztXiNOZM+nmUcz9mzv8tpTQfr5tISkoLfzUci8h1wm6r+q775WlJou8qrynl62dM89OVDHDh8AICEmATG9xnPlMFTGHbCMP6w+A/8NfOvtI9sz4S+E7j01EvJODGDXvG9iIqICvIatD6BTABNpaXEEWz+JgVUNSAvnE7sbKAX0A7IAgb6KHcqsA03QdX3Gj58uJq2o7q6WlfnrdbffvZbPemJk5QH0QmvT9Avt32pL3/3sk55f4rGPxKvPIiGzQjTsBlheufHd2pRWVGwQ28yPIhf073L+fpMffPxN4bGzMffz/hazvHEbf4NyFQ/jrGBviT1QmAmziWpL6nqH0XkITe4uW6ZB4FoVb3Pn3laTaH1Kywt5MllT/LNjm/I2p1Ffkk+YRLGT07+CXecfgcT+k44qnxZZRkfbvyQr3/8mhuG3sCQpCFBirzxGtrhGchOWtM6Bb35KFAsKbRexeXFvJb1Gn9Y/Af2le5j6AlDGdJ9CKf1OI3L+11O97juwQ6xSfl7hUxLiKs1La+tsqRgWqzi8mKeWvYUX/34FftK97G3ZC95xXmeK4LGpI5h5riZDE4aHORIGycU2tlN2xP0H6+Ztk1VyT2YS2FZIQcOH6C0opSK6go27t3Iw18/TF5xHoO7D6Z7XHdO7nyy53bEg5MGM673uOO6b3yw+TroH0kGlhBMS2dJwTS55TuXc99n97F422Kf00efNJr3rnmPM1PObObIjtWYHzQ15jOWDEyosOYj0yj7y/azfOdyVuetZm3+Ws+jBYvLi1mSs4Su7bty98i76d25Nx2iOtA+sj0RYRF0aNeBtG5pIVMTOJ42fmsqMi2J9SmYJlVZXclX27/i/Q3v81n2Z6zfu94zrXtsd1LjUwkPC0cQfnLyT7h75N10iOoQxIj/ral/qGVMKLI+BdNoOQdyWJS9iM+3fc6mgk0UlBSw69AuDpUfIjoimnNTz+XaQddyRvIZDEkaQkL7hGaPsSEHdV9n+pYQjPHNkoLx2LZ/G79b9DveXPMmAIntExmSNITU+FQSYxI5t9e5jOs9jth2sUGOtP6Duq+kYYnAmPpZUmjjisuL+WLbF8zbNI+XVr1EmIRx36j7mJQ2iUHdB4XkTdzAEoAxjRWa//HmuJVXlfP7Rb+ny5+7MPHNibya9So/G/QzNt++mYcveJjBSYMDkhBkhv8dzEfKen+mvs83ZP7GmGNZR3MboapkF2ZzuOowe4r3cOeCO8nKy+LaQdcyZfAUzup5VpM91rC572RpncbG1M86mo3H/rL9XP3O1XyW/ZlnXLfYbsyZNIdLTr2kyZfnqzO3qR964mt5xpjjZzWFVi67MJuJb0xky74tzBgzg5M7n0y78HZc8fYVx/3EKGNM6LCaQhtXUFLAsyueZebSmQAs/I+FjEkd45l+PL++ra8GYIwJXdbR3MqoKg9/9TAnzTyJ6V9MZ2TKSJZOXXpUQmiI+jpuLSEY07pYUmhFisuL+ensn/K7z3/H+D7jWX3raj762UecknBKrZ9pyEG/rrJ21Y8xrYM1H7USuw/tZvzr41m9ZzWP/uRR7hl5j1/3F2pIU5Dd8M2Y1s9qCq3AnuI9nP/a+WzZt4V5k+fx6zN/fVRC8Pcs3g7sxhirKYS4vSV7ueC1C9hauJWPr/2Yc1LPOaaMHeyNMf6ymkIIyzmQw7mvnsvmfZv5cPKHPhOCN2v3N8bUx5JCiFq7Zy0jXxzJ9v3bmTd5HueffP5R030lAKsxGGPqY81HIWj5zuWMe30cMRExfHnDlwxJGnJMGUsAxpjGsKQQYtbuWcuEWRPoHN2ZxVMW0zO+Z7BDMsa0ItZ8FEK2Fm5l7OtjiQqP4rPrP/OZEKzfwBhzPCwphIhD5YeYMGsCpRWlLPyPhfR+qrdnmncisGYjY8zxsOajEDFt/jQ2FWzi8ymfk9Yt7aiDvyUCY0xTsZpCCJj1/SxezXqV+8++n3NfPTfY4RhjWjFLCi1cdmE2t350K6NSRvHAOQ9YrcAYE1ABTQoiMl5ENorIFhG5r5YyPxWRdSKyVkTeCGQ8oehPX/2JyupKZl0xi4gwa+0zxgRWwI4yIhIOPAv8BMgBVojIXFVd51WmL/BbYJSqFopIt0DFE4r2luxl1upZXJ9+vV16aoxpFoGsKYwAtqhqtqqWA28Bl9YoczPwrKoWAqjqngDGE3Je+PYFyirLuP3024MdijGmjQhkUugB7PAaznHHeTsFOEVEvhGRpSIy3teMROQWEckUkcz8/PwAhduyVFZX8n8r/o9zU89l0F8GBTscY0wbEcik4OtXVDV7SSOAvsAYYDLwgojEH/Mh1edUNUNVM7p27drkgbZEczbMYceBHfzq9F9Z57IxptkEMinkAClew8lAro8yc1S1QlW3AhtxkkSb99Typ+jZqScXn3JxsEMxxrQhgUwKK4C+ItJLRNoBk4C5Ncp8AJwLICKJOM1J2QGMKSQs/GEhX27/kjtOv4PwsPBgh2OMaUMClhRUtRKYBnwCrAfeVtW1IvKQiFziFvsEKBCRdcBi4D9VtSBQMYWCyupK7vrkLnp37s0vT/tlsMMxxrQxAb3wXVXnA/NrjHvA670Cd7svA/w186+sy1/H+9e8T1REVLDDMca0MfaL5hZkX+k+pn8xnfN6ncelp9a8etcYYwLPkkIL8udv/sy+0n08Me4JROwW2MaY5mdJoYUoqSjhuZXPcdWAq0jvnh7scIwxbZTdTKeFiP1TLADTTpsW5EiMMW2ZJYUWQFUZ3H0w1VrN2T3PDnY4xpg2zJqPWoCvf/yarLwsbh9xu/UlGGOCypJCC/D08qeJj47nZ4N+FuxQjDFtnCWFIMs5kMN769/jpqE3EdsuNtjhGGPaOEsKQfb0sqdRlGkjrIPZGBN81tEcRDJDiI+O5/J+l5ManxrscIwxxmoKwfTMhGfYX7afu0faXT6MMS2DJYUgqdZqnlz2JCN6jGBk8shgh2OMMYAlhaCQGcJHmz5i877N3HXGXXYZqjGmxbA+hSDQ6crYv48luWMyV/a/MtjhGGOMh9UUgiC7MJtPsz/l5mE3ExkeGexwjDHGw5JCEDy/8nnCJIybht4U7FCMMeYolhSaWXlVOS+teomJp0ykR8cewQ7HGGOOYkmhmX248UP2FO/hlmG3BDsUY4w5hiWFZvbct8+R0jGF8X3GBzsUY4w5hiWFZrRt/zYW/rCQqcOmEh4WHuxwjDHmGJYUmtFba94CYMrgKUGOxBhjfLPfKTQTmSEMTRrKGcln0DO+Z7DDMcYYn6ym0Ew2TdvEd7u/46cDfhrsUIwxplaWFJrJO+veAeCqAVcFORJjjKmdJYVm8o+1/2BUyihSOqUEOxRjjKmVJYVmsGHvBr7P+56fDrSmI2NMyxbQpCAi40Vko4hsEZH7fEz/uYjki8gq9zU1kPEEyztr30EQazoyxrR4Abv6SETCgWeBnwA5wAoRmauq62oU/YeqttpnUVZWV/JK1iuc1fMsTuxwYrDDMcaYOgWypjAC2KKq2apaDrwFXBrA5bVI7657l+zCbO48/c5gh2KMMfUKZFLoAezwGs5xx9V0pYh8LyKzRcRnL6yI3CIimSKSmZ+fH4hYA0JVeeSbRzg14VQu7dfm8qExJgQFMin4epyY1hj+EEhV1XTgM+BVXzNS1edUNUNVM7p27drEYQZO2ENhrNq9intH3UuYWJ++MablC+SRKgfwPvNPBnK9C6hqgaoedgefB4YHMJ5mNyZ1DD069ODa9GuDHYoxxvglkElhBdBXRHqJSDtgEjDXu4CInOA1eAmwPoDxNKvM3Ey+2PYF94y8h3bh7YIdjjHG+CVgVx+paqWITAM+AcKBl1R1rYg8BGSq6lzgVyJyCVAJ7AN+Hqh4mtv7698nXMK5YegNwQ7FGGP8Jqo1m/lbtoyMDM3MzAx2GPU67fnTiI6I5qsbvgp2KMYYg4isVNWM+spZ72cA7C3Zy8rclYw9eWywQzHGmAaxW2cHQNdHnSukxva2pGCMCS1WUwiAnw/5OZ2jO5NxYr01NWOMaVEsKTQxVWXhDwu54OQL7JGbxpiQ41dSEJHeIhLlvh8jIr8SkfjAhhaa1uWvI/dgLuN6jwt2KMYY02D+1hTeBapEpA/wItALeCNgUYWwT374BICf9P5JkCMxxpiG8zcpVKtqJXA5MFNV7wJOqOczbYrMcO7qsfCHhfRL7MdJnU4KckTGGNNw/iaFChGZDEwB5rnjIgMTUmjS6UppRSn/3P5PuxTVGBOy/E0KNwAjgT+q6lYR6QW8HriwQtM/t/+TssoyJvSdEOxQjDGmUfz6nYL7YJxfAYhIZ6CDqj4SyMBC0YItC4iOiOacnucEOxRjjGkUf68++kJEOopIFyALeFlEHg9saKFnwZYFjEkdQ0xkTLBDMcaYRvG3+aiTqh4ArgBeVtXhwAWBCyv0bC3cysaCjYzvPT7YoRhjTKP5mxQi3Ntc/5R/dzQbLwu2LACw/gRjTEjzNyk8hHML7B9UdYWInAxsDlxYoWfBDwvoFd+Lvl36BjsUY4xpNH87mt8B3vEazgauDFRQoaa8qpxF2Yu4fvD1iPh6CqkxxoQGfzuak0XkfRHZIyJ5IvKuiCQHOrhQ8c2P31BcUcyEPtZ0ZIwJbf42H72M8yjNE4EewIfuuDZPZgifb/2ccAlnTOqYYIdjjDHHxd+k0FVVX1bVSvf1CtA1gHGFDJ2uLMlZQnr3dDpEdQh2OMYYc1z8TQp7ReQ6EQl3X9cBBYEMLFRUVVexfOdyRiaPDHYoxhhz3PxNCjfiXI66G9gFXIVz64s2b13+Og6WH2RkiiUFY0zo8yspqOqPqnqJqnZV1W6qehnOD9navCU5SwA4I/mMIEdijDHH73ievHZ3k0URwpbmLCWxfSJyuqPZAAAZGUlEQVS9O/cOdijGGHPcjicp2AX5ODWFM5LPsN8nGGNaheNJCtpkUYSowtJCNuzdYJ3MxphWo85fNIvIQXwf/AVo87cCXbZzGWD9CcaY1qPOpKCqduF9HZbsWEKYhDGix4hgh2KMMU3ieJqP2rylO5cyqNsg4trFBTsUY4xpEgFNCiIyXkQ2isgWEbmvjnJXiYiKSEYg42kqMkOoqq5iWc4yazoyxrQqAUsKIhIOPAtMAAYAk0VkgI9yHXAe9bksULE0NZ2uLNu5jKLDRZzX67xgh2OMMU0mkDWFEcAWVc1W1XLgLeBSH+X+C/gzUBbAWJrcR5s+IlzCGdt7bLBDMcaYJhPIpNAD2OE1nOOO8xCRoUCKqtb5NDcRuUVEMkUkMz8/v+kjbYR5m+cx+qTRxEfHBzsUY4xpMoFMCr5+zeW5vFVEwoAngHvqm5GqPqeqGaqa0bVr8G/OuqNoB9/nfc9FfS8KdijGGNOkApkUcoAUr+FkINdruAOQBnwhItuAM4C5odDZPH/zfAAuOsWSgjGmdQlkUlgB9BWRXiLSDpiE86AeAFS1SFUTVTVVVVOBpcAlqpoZwJiaxEebPyI1PpX+if2DHYoxxjSpgCUFVa0EpgGfAOuBt1V1rYg8JCKXBGq5gVZWWcairYu4qO9Fdr8jY0yrU+cvmo+Xqs4H5tcY90AtZccEMpam8sW2LyipKLH+BGNMq2S/aG6ghT8sJDoi2p7HbIxplSwpNNCK3BUMO2EYMZFt/n6AxphWyJJCA8gM4dtd35JxQou/QMoYYxrFkkIDrLl1DSUVJWScaEnBGNM6WVJogMxc52pZSwrGmNbKkkIDZOZmEtcujlMSTgl2KMYYExCWFBogc1cmw04YRnhYeLBDMcaYgLCk4KfK6kpW7V5lnczGmFbNkoKf1uWvo6yyzPoTjDGtmiUFP1knszGmLbCk4KfM3Ew6RXWid5fewQ7FGGMCxpKCnzJzMxl+4nDCxL4yY0zrZUc4P5RXlZOVl2WdzMaYVs+Sgh+ydmdRXlVu/QnGmFbPkoIfluQsAWBkysggR2KMMYFlScEPS3KWkNwxmeSOycEOxRhjAsqSgh+W7FjCyGSrJRhjWj9LCvXYdXAX24u2W1IwxrQJlhTqYf0Jxpi2xJJCPa58+0rahbdjaNLQYIdijDEBFxHsAFq6USmjqNZqoiKigh2KMcYEnNUU6lBeVU5mbqb1Jxhj2gxLCnVYtXsVh6sOW3+CMabNsKRQhyU73E5mqykYY9oISwp1WLpzKSkdU+jRsUewQzHGmGZhSaEOq3avYtgJw4IdhjHGNJuAJgURGS8iG0Vki4jc52P6L0RktYisEpGvRWRAIONpiNKKUjYVbGJw98HBDsUYY5pNwJKCiIQDzwITgAHAZB8H/TdUdZCqDgH+DDweqHgaas2eNVRrNYOTLCkYY9qOQNYURgBbVDVbVcuBt4BLvQuo6gGvwVhAAxhPg2TlZQFYTcEY06YE8sdrPYAdXsM5wOk1C4nIbcDdQDvgvADG0yBZu7OIaxdHr869gh2KMcY0m0DWFMTHuGNqAqr6rKr2Bu4F7vc5I5FbRCRTRDLz8/ObOEzfnlnxDOnd0+3xm8aYNiWQR7wcIMVrOBnIraP8W8Blviao6nOqmqGqGV27dm3CEH1TVTpFdbKmI2NMmxPIpLAC6CsivUSkHTAJmOtdQET6eg1eBGwOYDx+2160naLDRZYUjDFtTsD6FFS1UkSmAZ8A4cBLqrpWRB4CMlV1LjBNRC4AKoBCYEqg4mmIrN1uJ7NdeWSMaWMCepdUVZ0PzK8x7gGv93cEcvmNlZWXhSAM6jYo2KEYY0yzsl5UH7LysujTpQ+x7WKDHYoxxjQrSwo+ZO3OsqYjY0ybZEmhhoOHD/JD4Q/WyWyMaZMsKdSwctdKwH7JbIxpmywpeJEZwgcbPiAqPIoxqWOCHY4xxjQ7e0azl6oHqug5syfj+oyjQ1SHYIdjjDHNzmoKXlbsXEHOgRyu7H9lsEMxxpigsKTg5d317xIRFsHFp1wc7FCMMSYoLCm4VJV317/L+b3Op3NM52CHY4wxQWFJwZWVl0V2YbY1HRlj2rQ2nxRkhnOH73fXvUuYhHFZP583ajXGmDahzScFna6oKrPXz+acnufQNTbwt+Y2xpiWyi5JBdblr2PD3g3cPuL2YIdiTEBVVFSQk5NDWVlZsEMxARIdHU1ycjKRkZGN+rwlBWD2utkIwhX9rwh2KMYEVE5ODh06dCA1NRURXw9HNKFMVSkoKCAnJ4devRr3KOE233wEMHv9bM7qeRZJcUnBDsWYgCorKyMhIcESQislIiQkJBxXTbDNJoUjHcwb9m5gzZ41XNX/qiBHZEzzsITQuh3v9m2zSUGnK+A0HQHWdGSMMbThpHDE7HWzOTPlTHp07BHsUIxp9QoKChgyZAhDhgwhKSmJHj16eIbLy8v9mscNN9zAxo0b6yzz7LPPMmvWrKYIucndf//9zJw585jxU6ZMoWvXrgwZMiQIUf1bm+5o3lywmay8LB4f+3iwQzGmTUhISGDVqlUAPPjgg8TFxfHrX//6qDKqzmXiYWG+z1lffvnlepdz2223HX+wzezGG2/ktttu45ZbbglqHG06Kbz03UuESRhXDbD+BNP23LngTlbtXtWk8xySNISZ4489C67Pli1buOyyyxg9ejTLli1j3rx5zJgxg2+//ZbS0lKuueYaHnjAebz76NGjeeaZZ0hLSyMxMZFf/OIXfPzxx7Rv3545c+bQrVs37r//fhITE7nzzjsZPXo0o0eP5vPPP6eoqIiXX36ZM888k+LiYq6//nq2bNnCgAED2Lx5My+88MIxZ+rTp09n/vz5lJaWMnr0aP7yl78gImzatIlf/OIXFBQUEB4eznvvvUdqaip/+tOfePPNNwkLC2PixIn88Y9/9Os7OOecc9iyZUuDv7um1mabj0oqSvjbyr9xeb/LSemUEuxwjGnz1q1bx0033cR3331Hjx49eOSRR8jMzCQrK4tPP/2UdevWHfOZoqIizjnnHLKyshg5ciQvvfSSz3mrKsuXL+fRRx/loYceAuDpp58mKSmJrKws7rvvPr777jufn73jjjtYsWIFq1evpqioiAULFgAwefJk7rrrLrKysvjXv/5Ft27d+PDDD/n4449Zvnw5WVlZ3HPPPU307TSfNltTeC3rNQrLCrnzjDuDHYoxQdGYM/pA6t27N6eddppn+M033+TFF1+ksrKS3Nxc1q1bx4ABA476TExMDBMmTABg+PDhfPXVVz7nfcUVV3jKbNu2DYCvv/6ae++9F4DBgwczcOBAn59dtGgRjz76KGVlZezdu5fhw4dzxhlnsHfvXi6+2LmjcnR0NACfffYZN954IzExMQB06dKlMV9FULWppCAzBJ2uVGs1Ty57kowTMxiVMirYYRljgNjYWM/7zZs38+STT7J8+XLi4+O57rrrfF57365dO8/78PBwKisrfc47KirqmDKqWm9MJSUlTJs2jW+//ZYePXpw//33e+Lwdemnqob8Jb9tqvnoyGWoC39YyIa9G7jz9DtDfgMa0xodOHCADh060LFjR3bt2sUnn3zS5MsYPXo0b7/9NgCrV6/22TxVWlpKWFgYiYmJHDx4kHfffReAzp07k5iYyIcffgg4PwosKSlh7NixvPjii5SWlgKwb9++Jo870NpUUjjiyWVPckLcCVw98Opgh2KM8WHYsGEMGDCAtLQ0br75ZkaNavoa/e23387OnTtJT0/nscceIy0tjU6dOh1VJiEhgSlTppCWlsbll1/O6aef7pk2a9YsHnvsMdLT0xk9ejT5+flMnDiR8ePHk5GRwZAhQ3jiiSd8LvvBBx8kOTmZ5ORkUlNTAbj66qs566yzWLduHcnJybzyyitNvs7+EH+qUC1JRkaGZmZmNvrzldWVxP4plmmnTeOxcY81YWTGtHzr16+nf//+wQ6jRaisrKSyspLo6Gg2b97M2LFj2bx5MxERod+q7ms7i8hKVc2o77Ohv/YNtGXfFsqryhmSFNwfiBhjguvQoUOcf/75VFZWoqr87W9/axUJ4XgF9BsQkfHAk0A48IKqPlJj+t3AVKASyAduVNXtgYxpzZ41AKR1SwvkYowxLVx8fDwrV64MdhgtTsD6FEQkHHgWmAAMACaLyIAaxb4DMlQ1HZgN/DlQ8RyxOm81YRJGv8R+gV6UMcaEnEB2NI8AtqhqtqqWA28Bl3oXUNXFqlriDi4FkgMYDwBr8tfQp0sfYiJjAr0oY4wJOYFMCj2AHV7DOe642twEfOxrgojcIiKZIpKZn59/XEGt2bPGmo6MMaYWgUwKvn4A4PNSJxG5DsgAHvU1XVWfU9UMVc3o2rXxz1AurShly74tpHW1pGCMMb4EMinkAN43FUoGcmsWEpELgN8Dl6jq4QDGw4a9G6jWaqspGBMkY8aMOeaHaDNnzuSXv/xlnZ+Li4sDIDc3l6uu8n0DyzFjxlDf5eozZ86kpKTEM3zhhReyf/9+f0JvVl988QUTJ048ZvwzzzxDnz59EBH27t0bkGUHMimsAPqKSC8RaQdMAuZ6FxCRocDfcBLCngDGAtiVR8YE2+TJk3nrrbeOGvfWW28xefJkvz5/4oknMnv27EYvv2ZSmD9/PvHx8Y2eX3MbNWoUn332GT179gzYMgKWFFS1EpgGfAKsB95W1bUi8pCIXOIWexSIA94RkVUiMreW2TWJNXvW0C68HX0T+gZyMca0OkceX3u8rrrqKubNm8fhw06jwLZt28jNzWX06NGe3w0MGzaMQYMGMWfOnGM+v23bNtLSnJO60tJSJk2aRHp6Otdcc43n1hIAt956KxkZGQwcOJDp06cD8NRTT5Gbm8u5557LueeeC0BqaqrnjPvxxx8nLS2NtLQ0z0Nwtm3bRv/+/bn55psZOHAgY8eOPWo5R3z44YecfvrpDB06lAsuuIC8vDzA+S3EDTfcwKBBg0hPT/fcJmPBggUMGzaMwYMHc/755/v9/Q0dOtTzC+iAOfJAi1B5DR8+XBtrwusTdPBfBjf688aEunXr1gU7BL3wwgv1gw8+UFXVhx9+WH/961+rqmpFRYUWFRWpqmp+fr727t1bq6urVVU1NjZWVVW3bt2qAwcOVFXVxx57TG+44QZVVc3KytLw8HBdsWKFqqoWFBSoqmplZaWec845mpWVpaqqPXv21Pz8fE8sR4YzMzM1LS1NDx06pAcPHtQBAwbot99+q1u3btXw8HD97rvvVFX16quv1r///e/HrNO+ffs8sT7//PN69913q6rqb37zG73jjjuOKrdnzx5NTk7W7Ozso2L1tnjxYr3oootq/Q5rrkdNvrYzkKl+HGPb1L2P7MojY4LPuwnJu+lIVfnd735Heno6F1xwATt37vSccfvy5Zdfct111wGQnp5Oenq6Z9rbb7/NsGHDGDp0KGvXrvV5sztvX3/9NZdffjmxsbHExcVxxRVXeG7D3atXL8+Dd7xvve0tJyeHcePGMWjQIB599FHWrl0LOLfS9n4KXOfOnVm6dClnn302vXr1Alre7bXbTFIoKitix4EdlhSMCbLLLruMRYsWeZ6qNmzYMMC5wVx+fj4rV65k1apVdO/e3eftsr35usvx1q1b+d///V8WLVrE999/z0UXXVTvfLSOe8Adue021H577ttvv51p06axevVq/va3v3mWpz5upe1rXEvSZpLC2nwnc1tSMCa44uLiGDNmDDfeeONRHcxFRUV069aNyMhIFi9ezPbtdd/x5uyzz2bWrFkArFmzhu+//x5wbrsdGxtLp06dyMvL4+OP//3zpw4dOnDw4EGf8/rggw8oKSmhuLiY999/n7POOsvvdSoqKqJHD+dnWK+++qpn/NixY3nmmWc8w4WFhYwcOZJ//vOfbN26FWh5t9duM0nBrjwypuWYPHkyWVlZTJo0yTPu2muvJTMzk4yMDGbNmkW/fnXfiubWW2/l0KFDpKen8+c//5kRI0YAzlPUhg4dysCBA7nxxhuPuu32LbfcwoQJEzwdzUcMGzaMn//854wYMYLTTz+dqVOnMnToUL/X58EHH/Tc+joxMdEz/v7776ewsJC0tDQGDx7M4sWL6dq1K8899xxXXHEFgwcP5pprrvE5z0WLFnlur52cnMySJUt46qmnSE5OJicnh/T0dKZOnep3jP5qM7fOnrNhDi+vepn3rnmPMGkzudCYo9its9sGu3W2Hy7tdymX9ru0/oLGGNOG2SmzMcYYD0sKxrQxodZkbBrmeLevJQVj2pDo6GgKCgosMbRSqkpBQQHR0dGNnkeb6VMwxuC5cuV4b0FvWq7o6GiSkxv/aBpLCsa0IZGRkZ5f0hrjizUfGWOM8bCkYIwxxsOSgjHGGI+Q+0WziOQDdd8U5ViJQGAeU9T8bF1aJluXlqs1rc/xrEtPVa33ecYhlxQaQ0Qy/fl5dyiwdWmZbF1arta0Ps2xLtZ8ZIwxxsOSgjHGGI+2khSeC3YATcjWpWWydWm5WtP6BHxd2kSfgjHGGP+0lZqCMcYYP1hSMMYY49Gqk4KIjBeRjSKyRUTuC3Y8DSEiKSKyWETWi8haEbnDHd9FRD4Vkc3u387BjtVfIhIuIt+JyDx3uJeILHPX5R8i0i7YMfpLROJFZLaIbHC30chQ3TYicpe7j60RkTdFJDpUto2IvCQie0Rkjdc4n9tBHE+5x4PvRWRY8CI/Vi3r8qi7j30vIu+LSLzXtN+667JRRMY1VRytNimISDjwLDABGABMFpEBwY2qQSqBe1S1P3AGcJsb/33AIlXtCyxyh0PFHcB6r+H/AZ5w16UQuCkoUTXOk8ACVe0HDMZZr5DbNiLSA/gVkKGqaUA4MInQ2TavAONrjKttO0wA+rqvW4C/NFOM/nqFY9flUyBNVdOBTcBvAdxjwSRgoPuZ/3OPecet1SYFYASwRVWzVbUceAsImedxquouVf3WfX8Q56DTA2cdXnWLvQpcFpwIG0ZEkoGLgBfcYQHOA2a7RUJpXToCZwMvAqhquaruJ0S3Dc7dkmNEJAJoD+wiRLaNqn4J7KsxurbtcCnwmjqWAvEickLzRFo/X+uiqgtVtdIdXAocuSf2pcBbqnpYVbcCW3COecetNSeFHsAOr+Ecd1zIEZFUYCiwDOiuqrvASRxAt+BF1iAzgd8A1e5wArDfa4cPpe1zMpAPvOw2h70gIrGE4LZR1Z3A/wI/4iSDImAlobttoPbtEOrHhBuBj933AVuX1pwUxMe4kLv+VkTigHeBO1X1QLDjaQwRmQjsUdWV3qN9FA2V7RMBDAP+oqpDgWJCoKnIF7e9/VKgF3AiEIvTzFJTqGybuoTsPiciv8dpUp51ZJSPYk2yLq05KeQAKV7DyUBukGJpFBGJxEkIs1T1PXd03pEqr/t3T7Dia4BRwCUisg2nGe88nJpDvNtkAaG1fXKAHFVd5g7PxkkSobhtLgC2qmq+qlYA7wFnErrbBmrfDiF5TBCRKcBE4Fr99w/LArYurTkprAD6uldRtMPplJkb5Jj85ra5vwisV9XHvSbNBaa476cAc5o7toZS1d+qarKqpuJsh89V9VpgMXCVWywk1gVAVXcDO0TkVHfU+cA6QnDb4DQbnSEi7d197si6hOS2cdW2HeYC17tXIZ0BFB1pZmqpRGQ8cC9wiaqWeE2aC0wSkSgR6YXTeb68SRaqqq32BVyI02P/A/D7YMfTwNhH41QHvwdWua8LcdriFwGb3b9dgh1rA9drDDDPfX+yuyNvAd4BooIdXwPWYwiQ6W6fD4DOobptgBnABmAN8HcgKlS2DfAmTl9IBc7Z8021bQecJpdn3ePBapwrroK+DvWsyxacvoMjx4C/epX/vbsuG4EJTRWH3ebCGGOMR2tuPjLGGNNAlhSMMcZ4WFIwxhjjYUnBGGOMhyUFY4wxHpYUjHGJSJWIrPJ6NdmvlEUk1fvul8a0VBH1FzGmzShV1SHBDsKYYLKagjH1EJFtIvI/IrLcffVxx/cUkUXuve4XichJ7vju7r3vs9zXme6swkXkeffZBQtFJMYt/ysRWefO560graYxgCUFY7zF1Gg+usZr2gFVHQE8g3PfJtz3r6lzr/tZwFPu+KeAf6rqYJx7Iq11x/cFnlXVgcB+4Ep3/H3AUHc+vwjUyhnjD/tFszEuETmkqnE+xm8DzlPVbPcmhbtVNUFE9gInqGqFO36XqiaKSD6QrKqHveaRCnyqzoNfEJF7gUhV/W8RWQAcwrldxgeqeijAq2pMraymYIx/tJb3tZXx5bDX+yr+3ad3Ec49eYYDK73uTmpMs7OkYIx/rvH6u8R9/y+cu74CXAt87b5fBNwKnudSd6xtpiISBqSo6mKchxDFA8fUVoxpLnZGYsy/xYjIKq/hBap65LLUKBFZhnMiNdkd9yvgJRH5T5wnsd3gjr8DeE5EbsKpEdyKc/dLX8KB10WkE85dPJ9Q59GexgSF9SkYUw+3TyFDVfcGOxZjAs2aj4wxxnhYTcEYY4yH1RSMMcZ4WFIwxhjjYUnBGGOMhyUFY4wxHpYUjDHGePx/S33FCrDfmkQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 165us/step\n",
      "1500/1500 [==============================] - 0s 298us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.322066971206665, 0.7238666666984558]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3848867460886638, 0.6866666663487753]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 3s 449us/step - loss: 1.9890 - acc: 0.1433 - val_loss: 1.9353 - val_acc: 0.1700\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.9629 - acc: 0.1525 - val_loss: 1.9220 - val_acc: 0.1910\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 2s 207us/step - loss: 1.9418 - acc: 0.1579 - val_loss: 1.9131 - val_acc: 0.2080\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.9333 - acc: 0.1768 - val_loss: 1.9046 - val_acc: 0.2260\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 199us/step - loss: 1.9239 - acc: 0.1801 - val_loss: 1.8963 - val_acc: 0.2410\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 2s 208us/step - loss: 1.9156 - acc: 0.1939 - val_loss: 1.8870 - val_acc: 0.2550\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 2s 228us/step - loss: 1.9036 - acc: 0.2025 - val_loss: 1.8758 - val_acc: 0.2640\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 1s 181us/step - loss: 1.8971 - acc: 0.2040 - val_loss: 1.8630 - val_acc: 0.2870\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 1.8819 - acc: 0.2189 - val_loss: 1.8478 - val_acc: 0.3040\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 178us/step - loss: 1.8718 - acc: 0.2263 - val_loss: 1.8329 - val_acc: 0.3180\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 1s 171us/step - loss: 1.8604 - acc: 0.2376 - val_loss: 1.8157 - val_acc: 0.3270\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.8462 - acc: 0.2492 - val_loss: 1.7977 - val_acc: 0.3540\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 1s 156us/step - loss: 1.8286 - acc: 0.2540 - val_loss: 1.7773 - val_acc: 0.3740\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 1.8108 - acc: 0.2703 - val_loss: 1.7555 - val_acc: 0.3850\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.8002 - acc: 0.2711 - val_loss: 1.7340 - val_acc: 0.3940\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 1s 196us/step - loss: 1.7869 - acc: 0.2785 - val_loss: 1.7125 - val_acc: 0.4040\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 1.7650 - acc: 0.2961 - val_loss: 1.6885 - val_acc: 0.4160\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 2s 204us/step - loss: 1.7503 - acc: 0.2963 - val_loss: 1.6657 - val_acc: 0.4260\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 200us/step - loss: 1.7325 - acc: 0.3076 - val_loss: 1.6427 - val_acc: 0.4300\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 1.7198 - acc: 0.3135 - val_loss: 1.6189 - val_acc: 0.4430\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 1.7030 - acc: 0.3257 - val_loss: 1.5960 - val_acc: 0.4490\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 1.6817 - acc: 0.3337 - val_loss: 1.5709 - val_acc: 0.4620\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 1.6639 - acc: 0.3421 - val_loss: 1.5441 - val_acc: 0.4930\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 1s 164us/step - loss: 1.6461 - acc: 0.3528 - val_loss: 1.5208 - val_acc: 0.4970\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 1.6300 - acc: 0.3555 - val_loss: 1.4986 - val_acc: 0.5060\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 1.6070 - acc: 0.3629 - val_loss: 1.4725 - val_acc: 0.5130\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 1.5935 - acc: 0.3615 - val_loss: 1.4504 - val_acc: 0.5240\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 1.5737 - acc: 0.3821 - val_loss: 1.4272 - val_acc: 0.5460\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 173us/step - loss: 1.5612 - acc: 0.3872 - val_loss: 1.4055 - val_acc: 0.5450\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 1.5512 - acc: 0.3871 - val_loss: 1.3845 - val_acc: 0.5650\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 157us/step - loss: 1.5372 - acc: 0.3916 - val_loss: 1.3652 - val_acc: 0.5610\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 192us/step - loss: 1.5132 - acc: 0.4132 - val_loss: 1.3459 - val_acc: 0.5800\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 174us/step - loss: 1.5038 - acc: 0.4095 - val_loss: 1.3274 - val_acc: 0.5950\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 178us/step - loss: 1.4745 - acc: 0.4220 - val_loss: 1.3061 - val_acc: 0.6030\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 180us/step - loss: 1.4730 - acc: 0.4195 - val_loss: 1.2902 - val_acc: 0.6150\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.4644 - acc: 0.4281 - val_loss: 1.2739 - val_acc: 0.6240\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 166us/step - loss: 1.4366 - acc: 0.4484 - val_loss: 1.2540 - val_acc: 0.6230\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 195us/step - loss: 1.4238 - acc: 0.4532 - val_loss: 1.2369 - val_acc: 0.6290\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 2s 217us/step - loss: 1.3996 - acc: 0.4555 - val_loss: 1.2201 - val_acc: 0.6360\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.4037 - acc: 0.4572 - val_loss: 1.2044 - val_acc: 0.6400\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.4024 - acc: 0.4596 - val_loss: 1.1933 - val_acc: 0.6400\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 172us/step - loss: 1.3722 - acc: 0.4705 - val_loss: 1.1779 - val_acc: 0.6470\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 161us/step - loss: 1.3607 - acc: 0.4749 - val_loss: 1.1636 - val_acc: 0.6470\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.3508 - acc: 0.4813 - val_loss: 1.1489 - val_acc: 0.6580\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.3394 - acc: 0.4800 - val_loss: 1.1362 - val_acc: 0.6560\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 165us/step - loss: 1.3340 - acc: 0.4797 - val_loss: 1.1260 - val_acc: 0.6470\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.3354 - acc: 0.4860 - val_loss: 1.1172 - val_acc: 0.6560\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.3169 - acc: 0.4857 - val_loss: 1.1056 - val_acc: 0.6560\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 162us/step - loss: 1.2779 - acc: 0.5051 - val_loss: 1.0932 - val_acc: 0.6540\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.2858 - acc: 0.5083 - val_loss: 1.0831 - val_acc: 0.6620\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.2761 - acc: 0.5139 - val_loss: 1.0719 - val_acc: 0.6680\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 1s 142us/step - loss: 1.2734 - acc: 0.5080 - val_loss: 1.0621 - val_acc: 0.6690\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 163us/step - loss: 1.2651 - acc: 0.5139 - val_loss: 1.0510 - val_acc: 0.6730\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 1s 177us/step - loss: 1.2548 - acc: 0.5176 - val_loss: 1.0429 - val_acc: 0.6700\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.2300 - acc: 0.5253 - val_loss: 1.0308 - val_acc: 0.6710\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 158us/step - loss: 1.2286 - acc: 0.5367 - val_loss: 1.0211 - val_acc: 0.6800\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 154us/step - loss: 1.2205 - acc: 0.5356 - val_loss: 1.0144 - val_acc: 0.6710\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 1s 160us/step - loss: 1.2116 - acc: 0.5383 - val_loss: 1.0055 - val_acc: 0.6800\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 147us/step - loss: 1.2122 - acc: 0.5401 - val_loss: 0.9983 - val_acc: 0.6800\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 151us/step - loss: 1.1933 - acc: 0.5440 - val_loss: 0.9884 - val_acc: 0.6860\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 184us/step - loss: 1.1906 - acc: 0.5484 - val_loss: 0.9808 - val_acc: 0.6890\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 159us/step - loss: 1.1869 - acc: 0.5544 - val_loss: 0.9731 - val_acc: 0.6890\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 176us/step - loss: 1.1694 - acc: 0.5624 - val_loss: 0.9653 - val_acc: 0.6930\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 168us/step - loss: 1.1704 - acc: 0.5489 - val_loss: 0.9599 - val_acc: 0.6930\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 167us/step - loss: 1.1608 - acc: 0.5527 - val_loss: 0.9536 - val_acc: 0.6940\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 179us/step - loss: 1.1605 - acc: 0.5583 - val_loss: 0.9485 - val_acc: 0.6990\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 170us/step - loss: 1.1588 - acc: 0.5632 - val_loss: 0.9425 - val_acc: 0.6970\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 2s 209us/step - loss: 1.1359 - acc: 0.5715 - val_loss: 0.9339 - val_acc: 0.6960\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 1s 189us/step - loss: 1.1265 - acc: 0.5720 - val_loss: 0.9262 - val_acc: 0.6990\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 185us/step - loss: 1.1207 - acc: 0.5767 - val_loss: 0.9193 - val_acc: 0.6940\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 1s 182us/step - loss: 1.1107 - acc: 0.5784 - val_loss: 0.9129 - val_acc: 0.7030\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 188us/step - loss: 1.1337 - acc: 0.5715 - val_loss: 0.9086 - val_acc: 0.7040\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 1.1010 - acc: 0.5831 - val_loss: 0.9028 - val_acc: 0.7040\n",
      "Epoch 74/200\n",
      "1792/7500 [======>.......................] - ETA: 0s - loss: 1.1062 - acc: 0.5770"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9131 - acc: 0.1977 - val_loss: 1.8734 - val_acc: 0.2517\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.8204 - acc: 0.3034 - val_loss: 1.7551 - val_acc: 0.3397\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.6686 - acc: 0.4072 - val_loss: 1.5741 - val_acc: 0.4647\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.4662 - acc: 0.5248 - val_loss: 1.3619 - val_acc: 0.5560\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.2557 - acc: 0.6060 - val_loss: 1.1666 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0768 - acc: 0.6660 - val_loss: 1.0120 - val_acc: 0.6777\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.9451 - acc: 0.7012 - val_loss: 0.9037 - val_acc: 0.7047\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.8536 - acc: 0.7191 - val_loss: 0.8281 - val_acc: 0.7210\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7894 - acc: 0.7321 - val_loss: 0.7750 - val_acc: 0.7300\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7434 - acc: 0.7419 - val_loss: 0.7363 - val_acc: 0.7397\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7086 - acc: 0.7492 - val_loss: 0.7075 - val_acc: 0.7450\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6815 - acc: 0.7555 - val_loss: 0.6846 - val_acc: 0.7507\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6596 - acc: 0.7612 - val_loss: 0.6661 - val_acc: 0.7587\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6413 - acc: 0.7678 - val_loss: 0.6501 - val_acc: 0.7623\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6259 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7653\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6119 - acc: 0.7780 - val_loss: 0.6275 - val_acc: 0.7700\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5999 - acc: 0.7809 - val_loss: 0.6168 - val_acc: 0.7720\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5891 - acc: 0.7845 - val_loss: 0.6079 - val_acc: 0.7757\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5792 - acc: 0.7892 - val_loss: 0.6013 - val_acc: 0.7753\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5700 - acc: 0.7920 - val_loss: 0.5924 - val_acc: 0.7813\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5614 - acc: 0.7954 - val_loss: 0.5879 - val_acc: 0.7807\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5535 - acc: 0.7989 - val_loss: 0.5832 - val_acc: 0.7817\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5459 - acc: 0.8010 - val_loss: 0.5766 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5391 - acc: 0.8044 - val_loss: 0.5735 - val_acc: 0.7850\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5326 - acc: 0.8076 - val_loss: 0.5674 - val_acc: 0.7937\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8092 - val_loss: 0.5622 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5198 - acc: 0.8114 - val_loss: 0.5599 - val_acc: 0.7977\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5144 - acc: 0.8140 - val_loss: 0.5571 - val_acc: 0.8000\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5087 - acc: 0.8162 - val_loss: 0.5509 - val_acc: 0.8000\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5033 - acc: 0.8180 - val_loss: 0.5483 - val_acc: 0.8020\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4982 - acc: 0.8205 - val_loss: 0.5443 - val_acc: 0.8023\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4934 - acc: 0.8222 - val_loss: 0.5435 - val_acc: 0.8027\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4885 - acc: 0.8253 - val_loss: 0.5426 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4840 - acc: 0.8256 - val_loss: 0.5386 - val_acc: 0.8080\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4799 - acc: 0.8278 - val_loss: 0.5341 - val_acc: 0.8093\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4755 - acc: 0.8305 - val_loss: 0.5322 - val_acc: 0.8100\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4713 - acc: 0.8308 - val_loss: 0.5297 - val_acc: 0.8117\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4674 - acc: 0.8319 - val_loss: 0.5273 - val_acc: 0.8123\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4632 - acc: 0.8339 - val_loss: 0.5265 - val_acc: 0.8103\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8355 - val_loss: 0.5236 - val_acc: 0.8103\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8370 - val_loss: 0.5241 - val_acc: 0.8103\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4522 - acc: 0.8383 - val_loss: 0.5210 - val_acc: 0.8120\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4487 - acc: 0.8403 - val_loss: 0.5223 - val_acc: 0.8143\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4453 - acc: 0.8405 - val_loss: 0.5187 - val_acc: 0.8180\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4420 - acc: 0.8427 - val_loss: 0.5206 - val_acc: 0.8153\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8437 - val_loss: 0.5186 - val_acc: 0.8120\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4358 - acc: 0.8442 - val_loss: 0.5154 - val_acc: 0.8133\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4324 - acc: 0.8458 - val_loss: 0.5156 - val_acc: 0.8130\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4296 - acc: 0.8474 - val_loss: 0.5147 - val_acc: 0.8150\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8484 - val_loss: 0.5136 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8492 - val_loss: 0.5142 - val_acc: 0.8167\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4206 - acc: 0.8502 - val_loss: 0.5135 - val_acc: 0.8133\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4181 - acc: 0.8508 - val_loss: 0.5154 - val_acc: 0.8163\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4152 - acc: 0.8521 - val_loss: 0.5109 - val_acc: 0.8140\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.5124 - val_acc: 0.8160\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8537 - val_loss: 0.5126 - val_acc: 0.8163\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8543 - val_loss: 0.5112 - val_acc: 0.8180\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4052 - acc: 0.8555 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4028 - acc: 0.8556 - val_loss: 0.5111 - val_acc: 0.8130\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4000 - acc: 0.8580 - val_loss: 0.5105 - val_acc: 0.8183\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3977 - acc: 0.8590 - val_loss: 0.5116 - val_acc: 0.8163\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8602 - val_loss: 0.5123 - val_acc: 0.8180\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8606 - val_loss: 0.5089 - val_acc: 0.8157\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8617 - val_loss: 0.5120 - val_acc: 0.8113\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3888 - acc: 0.8624 - val_loss: 0.5128 - val_acc: 0.8157\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8633 - val_loss: 0.5122 - val_acc: 0.8167\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8638 - val_loss: 0.5100 - val_acc: 0.8157\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3821 - acc: 0.8646 - val_loss: 0.5113 - val_acc: 0.8160\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3801 - acc: 0.8665 - val_loss: 0.5136 - val_acc: 0.8120\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8661 - val_loss: 0.5121 - val_acc: 0.8163\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8688 - val_loss: 0.5113 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8685 - val_loss: 0.5115 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8689 - val_loss: 0.5121 - val_acc: 0.8167\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5157 - val_acc: 0.8160\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3681 - acc: 0.8702 - val_loss: 0.5137 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8725 - val_loss: 0.5126 - val_acc: 0.8143\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8718 - val_loss: 0.5138 - val_acc: 0.8147\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3627 - acc: 0.8725 - val_loss: 0.5194 - val_acc: 0.8160\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3610 - acc: 0.8739 - val_loss: 0.5152 - val_acc: 0.8117\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3589 - acc: 0.8742 - val_loss: 0.5166 - val_acc: 0.8170\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3571 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8765 - val_loss: 0.5159 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3536 - acc: 0.8771 - val_loss: 0.5180 - val_acc: 0.8157\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8768 - val_loss: 0.5189 - val_acc: 0.8140\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3501 - acc: 0.8779 - val_loss: 0.5177 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8785 - val_loss: 0.5218 - val_acc: 0.8167\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3468 - acc: 0.8794 - val_loss: 0.5212 - val_acc: 0.8137\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3456 - acc: 0.8793 - val_loss: 0.5198 - val_acc: 0.8153\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3438 - acc: 0.8801 - val_loss: 0.5210 - val_acc: 0.8143\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8802 - val_loss: 0.5235 - val_acc: 0.8127\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3406 - acc: 0.8813 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3386 - acc: 0.8818 - val_loss: 0.5223 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3374 - acc: 0.8827 - val_loss: 0.5232 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.8840 - val_loss: 0.5240 - val_acc: 0.8150\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8832 - val_loss: 0.5284 - val_acc: 0.8160\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3328 - acc: 0.8852 - val_loss: 0.5263 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.8856 - val_loss: 0.5260 - val_acc: 0.8137\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3297 - acc: 0.8869 - val_loss: 0.5322 - val_acc: 0.8117\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8863 - val_loss: 0.5297 - val_acc: 0.8140\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3257 - acc: 0.8878 - val_loss: 0.5295 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3239 - acc: 0.8889 - val_loss: 0.5335 - val_acc: 0.8143\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8888 - val_loss: 0.5320 - val_acc: 0.8153\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3212 - acc: 0.8890 - val_loss: 0.5335 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8891 - val_loss: 0.5339 - val_acc: 0.8150\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8903 - val_loss: 0.5370 - val_acc: 0.8143\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3171 - acc: 0.8912 - val_loss: 0.5352 - val_acc: 0.8147\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.5379 - val_acc: 0.8127\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8923 - val_loss: 0.5363 - val_acc: 0.8137\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8931 - val_loss: 0.5379 - val_acc: 0.8133\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.8927 - val_loss: 0.5388 - val_acc: 0.8153\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8924 - val_loss: 0.5392 - val_acc: 0.8147\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8947 - val_loss: 0.5406 - val_acc: 0.8137\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8943 - val_loss: 0.5422 - val_acc: 0.8157\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8951 - val_loss: 0.5433 - val_acc: 0.8123\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8956 - val_loss: 0.5432 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.5483 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8965 - val_loss: 0.5461 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3013 - acc: 0.8966 - val_loss: 0.5459 - val_acc: 0.8127\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8972 - val_loss: 0.5460 - val_acc: 0.8150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
